{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1d4aee4a0df2481ca910237b0272ecb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_601baf2be6074c3fb864689b2a827da5",
              "IPY_MODEL_35a01a9700b948e4ae198aab86e34246",
              "IPY_MODEL_40462397ba754771bad03f450f1fe09a"
            ],
            "layout": "IPY_MODEL_83ade269cec642b0a71024c4426d8d74"
          }
        },
        "601baf2be6074c3fb864689b2a827da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdc6196d02b7472b85f109e60594112a",
            "placeholder": "​",
            "style": "IPY_MODEL_b2aba34509d3426a91461fa5e06dfc8b",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "35a01a9700b948e4ae198aab86e34246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8713ff909b2145f7a93f03c7c4a4f5f0",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99c91ff5f1ec4983aab45a2f163faaba",
            "value": 160
          }
        },
        "40462397ba754771bad03f450f1fe09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99a35e887a8e4a9d8de89bb86326c01e",
            "placeholder": "​",
            "style": "IPY_MODEL_4ab743f11fcd4a7c9b310713acf31991",
            "value": " 160/160 [00:00&lt;00:00, 14.3kB/s]"
          }
        },
        "83ade269cec642b0a71024c4426d8d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdc6196d02b7472b85f109e60594112a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2aba34509d3426a91461fa5e06dfc8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8713ff909b2145f7a93f03c7c4a4f5f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c91ff5f1ec4983aab45a2f163faaba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99a35e887a8e4a9d8de89bb86326c01e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ab743f11fcd4a7c9b310713acf31991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruCMrPyitA-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f30a6978-2596-4b61-ec50-b7db29319d43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import zip_longest\n",
        "from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import sys\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1"
      ],
      "metadata": {
        "id": "iD_c7Ek1tRyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset extraction"
      ],
      "metadata": {
        "id": "-QawSPhrYi0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/aait-asg-2-task-1.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "id": "EvDm7-NXx94T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/content/data/task1/train_data/images/labeled')))\n",
        "print(len(os.listdir('/content/data/task1/train_data/images/unlabeled')))\n",
        "print(len(os.listdir('/content/data/task1/val_data')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "VGtK3HR3mJSb",
        "outputId": "dcd053a9-1b8e-4ea0-f472-8ddd7037ec0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e3a3c075a213>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/task1/train_data/images/labeled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/task1/train_data/images/unlabeled'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/data/task1/val_data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_file = \"/content/data/task1/train_data/annotations.csv\"\n",
        "\n",
        "annotations = pd.read_csv(annotations_file)\n",
        "\n",
        "print(f\"Total number of entries in the annotations file: {len(annotations)}\")\n",
        "\n",
        "annotations.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "MG9YYJn9M2EA",
        "outputId": "10261c22-e623-461c-96d1-b99abdc4a2da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of entries in the annotations file: 23555\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   sample  label\n",
              "0  task1/train_data/images/labeled/0.jpeg      0\n",
              "1  task1/train_data/images/labeled/1.jpeg      1\n",
              "2  task1/train_data/images/labeled/2.jpeg      2\n",
              "3  task1/train_data/images/labeled/3.jpeg      3\n",
              "4  task1/train_data/images/labeled/4.jpeg      4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e4e88656-d9d6-4239-94c6-80827fcf74a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>task1/train_data/images/labeled/0.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>task1/train_data/images/labeled/1.jpeg</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>task1/train_data/images/labeled/2.jpeg</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>task1/train_data/images/labeled/3.jpeg</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>task1/train_data/images/labeled/4.jpeg</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4e88656-d9d6-4239-94c6-80827fcf74a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e4e88656-d9d6-4239-94c6-80827fcf74a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e4e88656-d9d6-4239-94c6-80827fcf74a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0b45c36-a398-44f6-a097-74add8ce48a4\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0b45c36-a398-44f6-a097-74add8ce48a4')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0b45c36-a398-44f6-a097-74add8ce48a4 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations",
              "summary": "{\n  \"name\": \"annotations\",\n  \"rows\": 23555,\n  \"fields\": [\n    {\n      \"column\": \"sample\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23555,\n        \"samples\": [\n          \"task1/train_data/images/labeled/4934.jpeg\",\n          \"task1/train_data/images/labeled/4013.jpeg\",\n          \"task1/train_data/images/labeled/1056.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = annotations['label'].value_counts().sort_index()\n",
        "\n",
        "# Plot class distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(class_counts.index, class_counts.values)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in Training Data')\n",
        "plt.xticks(range(len(class_counts)))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "rgQxKyrrNQXG",
        "outputId": "05282ad1-b539-4e32-9019-e6162069509a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full supervised learning"
      ],
      "metadata": {
        "id": "eD8ssZz0Yqi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_path = \"/content/data/task1/train_data/annotations.csv\"\n",
        "labeled_images_path = \"/content/data/task1/train_data/images/labeled\"\n",
        "unlabeled_images_path = \"/content/data/task1/train_data/images/unlabeled\"\n",
        "val_images_path = \"/content/data/task1/val_data\"\n",
        "\n",
        "# Load annotations\n",
        "annotations = pd.read_csv(annotations_path)\n",
        "# Split into train and val\n",
        "train_annotations, val_annotations = train_test_split(annotations, test_size=0.2, stratify=annotations['label'], random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_annotations)}, Validation samples: {len(val_annotations)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6_L4yOhA5r0",
        "outputId": "a80184e4-1151-41f6-a389-18ff9f88fa51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 18844, Validation samples: 4711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, annotations, image_dir, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations.iloc[idx, 0]\n",
        "        label = self.annotations.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name"
      ],
      "metadata": {
        "id": "bFlS0EbCBIYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = LabeledDataset(train_annotations, \"/content/data\", transform=train_transforms)\n",
        "val_dataset = LabeledDataset(val_annotations, \"/content/data\", transform=val_transforms)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jKysYuUBc-E",
        "outputId": "61433122-478c-45be-d90a-87591397fb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training batches: 589, Validation batches: 148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=\"DEFAULT\")\n",
        "num_classes = len(annotations['label'].unique())\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3fG7G_BB9Wo",
        "outputId": "c1360fc1-67a4-457b-f4ef-f13f1cf1d463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 198MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Training on labeled data\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * correct / total\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS4JycuyCZdZ",
        "outputId": "c6d3f316-93b2-41ce-df77-3fb2633531fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 3.8079, Train Acc: 12.37%, Val Loss: 3.6270, Val Acc: 16.64%\n",
            "Epoch 2/10, Train Loss: 3.2504, Train Acc: 22.19%, Val Loss: 2.8999, Val Acc: 29.44%\n",
            "Epoch 3/10, Train Loss: 2.9479, Train Acc: 27.76%, Val Loss: 2.6354, Val Acc: 35.13%\n",
            "Epoch 4/10, Train Loss: 2.7325, Train Acc: 32.45%, Val Loss: 2.4632, Val Acc: 37.95%\n",
            "Epoch 5/10, Train Loss: 2.5700, Train Acc: 36.32%, Val Loss: 2.2980, Val Acc: 40.88%\n",
            "Epoch 6/10, Train Loss: 2.4144, Train Acc: 39.41%, Val Loss: 2.2994, Val Acc: 41.90%\n",
            "Epoch 7/10, Train Loss: 2.2987, Train Acc: 41.90%, Val Loss: 2.2302, Val Acc: 43.60%\n",
            "Epoch 8/10, Train Loss: 2.1753, Train Acc: 44.41%, Val Loss: 2.0881, Val Acc: 47.57%\n",
            "Epoch 9/10, Train Loss: 2.0837, Train Acc: 46.29%, Val Loss: 2.0556, Val Acc: 48.52%\n",
            "Epoch 10/10, Train Loss: 1.9841, Train Acc: 49.16%, Val Loss: 1.9812, Val Acc: 50.22%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"10_model.pth\")\n"
      ],
      "metadata": {
        "id": "jrbw38W4K-GU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # Training on labeled data\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * correct / total\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYMdZQoWK4mj",
        "outputId": "c2f5bae4-8f27-452a-88c3-4049b9366494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 1.8902, Train Acc: 51.05%, Val Loss: 1.9930, Val Acc: 50.84%\n",
            "Epoch 2/10, Train Loss: 1.7969, Train Acc: 53.38%, Val Loss: 1.9531, Val Acc: 51.69%\n",
            "Epoch 3/10, Train Loss: 1.7387, Train Acc: 54.37%, Val Loss: 1.9244, Val Acc: 53.13%\n",
            "Epoch 4/10, Train Loss: 1.6576, Train Acc: 56.68%, Val Loss: 2.0039, Val Acc: 53.22%\n",
            "Epoch 5/10, Train Loss: 1.5750, Train Acc: 58.59%, Val Loss: 1.8960, Val Acc: 54.00%\n",
            "Epoch 6/10, Train Loss: 1.5017, Train Acc: 60.23%, Val Loss: 2.0430, Val Acc: 52.54%\n",
            "Epoch 7/10, Train Loss: 1.4375, Train Acc: 61.88%, Val Loss: 2.0566, Val Acc: 51.31%\n",
            "Epoch 8/10, Train Loss: 1.3772, Train Acc: 63.30%, Val Loss: 1.9268, Val Acc: 54.53%\n",
            "Epoch 9/10, Train Loss: 1.3121, Train Acc: 65.20%, Val Loss: 2.1435, Val Acc: 52.22%\n",
            "Epoch 10/10, Train Loss: 1.2517, Train Acc: 66.40%, Val Loss: 2.0115, Val Acc: 54.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"20_model.pth\")"
      ],
      "metadata": {
        "id": "PefKzcUQNknr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dir = \"/content/data/task1/val_data\"\n",
        "\n",
        "# Create the validation dataset and DataLoader\n",
        "test_dataset = UnlabeledDataset(val_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "all_files = sorted(os.listdir(val_dir))\n",
        "\n",
        "# Check for duplicates\n",
        "duplicates = set([file for file in all_files if all_files.count(file) > 1])\n",
        "if duplicates:\n",
        "    print(f\"Duplicate files found: {duplicates}\")\n",
        "else:\n",
        "    print(\"No duplicates in val_data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ru7JQKGvOdsw",
        "outputId": "e8914aa0-e724-4d73-b8f4-04a5a834e267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicates in val_data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# List to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Inference loop\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
        "\n",
        "        # Append predictions with image names\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\n",
        "                \"ID\": f\"task1/val_data/{img_name}\",\n",
        "                \"label\": label\n",
        "            })\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "\n",
        "# Save to CSV\n",
        "submission_file = \"submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "\n",
        "print(f\"Submission file saved as {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5FZi6ckOOHf",
        "outputId": "d51824f0-2f4b-4b9f-d49e-1833b0dbb708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved as submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate IDs\n",
        "if submission_df[\"ID\"].duplicated().any():\n",
        "    print(\"Duplicate IDs found in submission file!\")\n",
        "    duplicates = submission_df[submission_df[\"ID\"].duplicated(keep=False)]\n",
        "    print(duplicates)\n",
        "else:\n",
        "    print(\"No duplicate IDs in submission file.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RK6kkaPXQcaR",
        "outputId": "647a5332-c887-409a-b564-ef4a514a28fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicate IDs in submission file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FixMatch"
      ],
      "metadata": {
        "id": "WBF7oz0mVg_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset for Labeled Data\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, annotations_df, image_dir, transform=None):\n",
        "        self.annotations_df = annotations_df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations_df.iloc[idx, 0]\n",
        "        label = self.annotations_df.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Custom Dataset for Unlabeled Data\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, weak_transform=None, strong_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.weak_transform = weak_transform\n",
        "        self.strong_transform = strong_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        weak_image = self.weak_transform(image) if self.weak_transform else image\n",
        "        strong_image = self.strong_transform(image) if self.strong_transform else image\n",
        "\n",
        "        return weak_image, strong_image, img_name"
      ],
      "metadata": {
        "id": "A99zkpNPV2mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fixmatch_loss(weak_logits, strong_logits, threshold=0.80):\n",
        "    \"\"\"\n",
        "    Consistency regularization loss for FixMatch.\n",
        "    Args:\n",
        "        weak_logits: Logits from weakly augmented images.\n",
        "        strong_logits: Logits from strongly augmented images.\n",
        "        threshold: Confidence threshold for pseudo-labeling.\n",
        "    Returns:\n",
        "        Consistency loss for unlabeled data.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    probs = F.softmax(weak_logits, dim=1)\n",
        "    pseudo_labels = torch.argmax(probs, dim=1)\n",
        "    mask = probs.max(dim=1)[0] >= threshold  # Confidence mask\n",
        "\n",
        "    # # Debugging output\n",
        "    # print(f\"Mask sum: {mask.sum().item()}, Total samples: {mask.size(0)}\")\n",
        "    # print(f\"Probabilities max: {probs.max(dim=1)[0]}\")\n",
        "\n",
        "    if mask.sum().item() == 0:  # No samples meet the confidence threshold\n",
        "        return torch.tensor(0.0, requires_grad=True).to(weak_logits.device)\n",
        "\n",
        "    return F.cross_entropy(strong_logits[mask], pseudo_labels[mask])"
      ],
      "metadata": {
        "id": "-vxEMPCTZXAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weak augmentation (basic resizing and flipping)\n",
        "weak_augment = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Strong augmentation (color jitter, random crop, etc.)\n",
        "strong_augment = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Augmentation for labeled data\n",
        "train_augment = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Load labeled data\n",
        "annotations_path = \"/content/data/task1/train_data/annotations.csv\"\n",
        "annotations_df = pd.read_csv(annotations_path)\n",
        "\n",
        "labeled_dataset = LabeledDataset(annotations_df, \"/content/data\", transform=train_augment)\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Load unlabeled data\n",
        "unlabeled_images_dir = \"/content/data/task1/train_data/images/unlabeled\"\n",
        "unlabeled_dataset = UnlabeledDataset(unlabeled_images_dir, weak_transform=weak_augment, strong_transform=strong_augment)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "I6OljR2bVv9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_path = \"/content/10_model.pth\"\n",
        "\n",
        "model = models.resnet50(weights=None)  # Use pretrained=False since you've trained it already\n",
        "num_classes = len(annotations_df['label'].unique())\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "print(num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_path))\n",
        "print(\"Checkpoint loaded successfully!\")\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0PrsdcGJaBB2",
        "outputId": "d02517b8-ac16-4c70-b3b6-68d825c45568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "Checkpoint loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-1bf1a1b0b297>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "lambda_u_schedule = lambda epoch: min(0.5, epoch / 10)  # Full weight of 0.5 after 10 epochs\n",
        "\n",
        "initial_threshold = 0.70\n",
        "final_threshold = 0.95\n",
        "threshold_schedule = lambda epoch: initial_threshold + (final_threshold - initial_threshold) * (epoch / num_epochs)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, supervised_loss, unsupervised_loss = 0, 0, 0\n",
        "    lambda_u = lambda_u_schedule(epoch)\n",
        "    threshold = threshold_schedule(epoch)\n",
        "\n",
        "\n",
        "    for (labeled_batch, unlabeled_batch) in zip_longest(labeled_loader, unlabeled_loader, fillvalue=None):\n",
        "        if labeled_batch is None or unlabeled_batch is None:\n",
        "          continue  # Skip incomplete batches\n",
        "\n",
        "        labeled_imgs, labeled_labels = labeled_batch\n",
        "        weak_imgs, strong_imgs, _ = unlabeled_batch\n",
        "        # Labeled data\n",
        "        labeled_imgs, labeled_labels = labeled_imgs.to(device), labeled_labels.to(device)\n",
        "        labeled_logits = model(labeled_imgs)\n",
        "        loss_supervised = criterion(labeled_logits, labeled_labels)\n",
        "\n",
        "        # Unlabeled data\n",
        "        weak_imgs, strong_imgs = weak_imgs.to(device), strong_imgs.to(device)\n",
        "        weak_logits = model(weak_imgs)\n",
        "        strong_logits = model(strong_imgs)\n",
        "        loss_unsupervised = fixmatch_loss(weak_logits, strong_logits, threshold=threshold)\n",
        "\n",
        "        # Combine losses\n",
        "        loss = loss_supervised + lambda_u * loss_unsupervised\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        supervised_loss += loss_supervised.item()\n",
        "        unsupervised_loss += loss_unsupervised.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Lambda_u: {lambda_u:.2f}, Threshold: {threshold:.2f}, \"\n",
        "          f\"Total Loss: {total_loss:.4f}, Supervised Loss: {supervised_loss:.4f}, Unsupervised Loss: {unsupervised_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVfNhrQcaUMQ",
        "outputId": "cc4bd008-7f17-432e-84fe-60c53ab4ea1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Lambda_u: 0.00, Threshold: 0.70, Total Loss: 1491.0569, Supervised Loss: 1491.0569, Unsupervised Loss: 566.5266\n",
            "Epoch 2/10, Lambda_u: 0.10, Threshold: 0.72, Total Loss: 1447.7164, Supervised Loss: 1392.4243, Unsupervised Loss: 552.9207\n",
            "Epoch 3/10, Lambda_u: 0.20, Threshold: 0.75, Total Loss: 1413.2791, Supervised Loss: 1307.5684, Unsupervised Loss: 528.5531\n",
            "Epoch 4/10, Lambda_u: 0.30, Threshold: 0.77, Total Loss: 1376.9800, Supervised Loss: 1226.8890, Unsupervised Loss: 500.3034\n",
            "Epoch 5/10, Lambda_u: 0.40, Threshold: 0.80, Total Loss: 1318.3026, Supervised Loss: 1124.4493, Unsupervised Loss: 484.6333\n",
            "Epoch 6/10, Lambda_u: 0.50, Threshold: 0.82, Total Loss: 1256.2495, Supervised Loss: 1024.2525, Unsupervised Loss: 463.9939\n",
            "Epoch 7/10, Lambda_u: 0.50, Threshold: 0.85, Total Loss: 1144.6912, Supervised Loss: 934.5553, Unsupervised Loss: 420.2719\n",
            "Epoch 8/10, Lambda_u: 0.50, Threshold: 0.88, Total Loss: 1036.7149, Supervised Loss: 840.0357, Unsupervised Loss: 393.3585\n",
            "Epoch 9/10, Lambda_u: 0.50, Threshold: 0.90, Total Loss: 955.4654, Supervised Loss: 774.3190, Unsupervised Loss: 362.2929\n",
            "Epoch 10/10, Lambda_u: 0.50, Threshold: 0.92, Total Loss: 910.7817, Supervised Loss: 743.0641, Unsupervised Loss: 335.4352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"fix10_model.pth\")"
      ],
      "metadata": {
        "id": "vlOA2UwYvmuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "additional_epochs = 5\n",
        "total_epochs = 10 + additional_epochs\n",
        "\n",
        "lambda_u_schedule = lambda epoch: min(0.5, epoch / total_epochs)\n",
        "threshold_schedule = lambda epoch: initial_threshold + (final_threshold - initial_threshold) * (epoch / total_epochs)\n",
        "\n",
        "# Resume training\n",
        "for epoch in range(10, 15):  # Start from epoch 11\n",
        "    model.train()\n",
        "    total_loss, supervised_loss, unsupervised_loss = 0, 0, 0\n",
        "    lambda_u = lambda_u_schedule(epoch)\n",
        "    threshold = threshold_schedule(epoch)\n",
        "\n",
        "    for (labeled_batch, unlabeled_batch) in zip_longest(labeled_loader, unlabeled_loader, fillvalue=None):\n",
        "        if labeled_batch is None or unlabeled_batch is None:\n",
        "            continue  # Skip incomplete batches\n",
        "\n",
        "        # Prepare data\n",
        "        labeled_imgs, labeled_labels = labeled_batch\n",
        "        weak_imgs, strong_imgs, _ = unlabeled_batch\n",
        "\n",
        "        # Labeled data\n",
        "        labeled_imgs, labeled_labels = labeled_imgs.to(device), labeled_labels.to(device)\n",
        "        labeled_logits = model(labeled_imgs)\n",
        "        loss_supervised = criterion(labeled_logits, labeled_labels)\n",
        "\n",
        "        # Unlabeled data\n",
        "        weak_imgs, strong_imgs = weak_imgs.to(device), strong_imgs.to(device)\n",
        "        weak_logits = model(weak_imgs)\n",
        "        strong_logits = model(strong_imgs)\n",
        "        loss_unsupervised = fixmatch_loss(weak_logits, strong_logits, threshold=threshold)\n",
        "\n",
        "        # Combine losses\n",
        "        loss = loss_supervised + lambda_u * loss_unsupervised\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Accumulate losses\n",
        "        total_loss += loss.item()\n",
        "        supervised_loss += loss_supervised.item()\n",
        "        unsupervised_loss += loss_unsupervised.item()\n",
        "\n",
        "    # Scheduler step\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print epoch results\n",
        "    print(f\"Epoch {epoch + 1}/{total_epochs}, Lambda_u: {lambda_u:.2f}, Threshold: {threshold:.2f}, \"\n",
        "          f\"Total Loss: {total_loss:.4f}, Supervised Loss: {supervised_loss:.4f}, Unsupervised Loss: {unsupervised_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSfgj8SPwCme",
        "outputId": "9d3ed3af-ccbb-44cb-d899-8069fb114393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15, Lambda_u: 0.50, Threshold: 0.87, Total Loss: 921.4260, Supervised Loss: 732.5806, Unsupervised Loss: 377.6909\n",
            "Epoch 12/15, Lambda_u: 0.50, Threshold: 0.88, Total Loss: 927.2337, Supervised Loss: 737.8690, Unsupervised Loss: 378.7294\n",
            "Epoch 13/15, Lambda_u: 0.50, Threshold: 0.90, Total Loss: 934.3778, Supervised Loss: 752.5072, Unsupervised Loss: 363.7412\n",
            "Epoch 14/15, Lambda_u: 0.50, Threshold: 0.92, Total Loss: 939.7047, Supervised Loss: 770.6996, Unsupervised Loss: 338.0103\n",
            "Epoch 15/15, Lambda_u: 0.50, Threshold: 0.93, Total Loss: 958.8840, Supervised Loss: 786.6824, Unsupervised Loss: 344.4033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to the validation directory\n",
        "val_dir = \"/content/data/task1/val_data\"\n",
        "\n",
        "# Define transformations (same as weak augmentations used during training)\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Custom Dataset for Validation Data\n",
        "class ValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create the validation dataset and dataloader\n",
        "val_dataset = ValDataset(val_dir, transform=val_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Load the trained model\n",
        "model = models.resnet50(weights=None)  # Initialize model\n",
        "num_classes = len(annotations_df['label'].unique())  # Use the correct number of classes\n",
        "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
        "model.load_state_dict(torch.load(\"/content/fix10_model.pth\"))  # Load checkpoint\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)  # Get class predictions\n",
        "\n",
        "        # Collect predictions with filenames\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": label})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Predictions saved to {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moDBvyqf0Q3X",
        "outputId": "e368afff-340e-4994-d626-cf62fa3eeb1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-4d013016cee3>:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/fix10_model.pth\"))  # Load checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViT"
      ],
      "metadata": {
        "id": "hzcL83eK2vyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize model and feature extractor\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"\n",
        "model = ViTForImageClassification.from_pretrained(model_name, num_labels=100)  # Adjust num_labels as needed\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQAZQVVP3GBQ",
        "outputId": "b65409ef-6e31-4ac4-9850-bd6f597f9b14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_df, image_dir, transform=None):\n",
        "        self.annotations_df = annotations_df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations_df.iloc[idx, 0]\n",
        "        label = self.annotations_df.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "6rFl7Hyx3hXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"./checkpoints/\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "\n",
        "annotations_file = \"/content/data/task1/train_data/annotations.csv\"\n",
        "\n",
        "annotations_df = pd.read_csv(annotations_file)\n",
        "\n",
        "# Stratified split (preserves class distribution)\n",
        "train_annotations, val_annotations = train_test_split(annotations_df, test_size=0.2, stratify=annotations_df['label'], random_state=42)\n",
        "\n",
        "train_dataset = CustomDataset(train_annotations, \"/content/data\", transform=train_transform)\n",
        "val_dataset = CustomDataset(val_annotations, \"/content/data\", transform=val_transform)\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.logits.max(1)  # Access logits\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * correct / total\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.logits, labels)  # Use logits for validation\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.logits.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "    # Update the scheduler\n",
        "    scheduler.step()\n",
        "\n",
        "    # Print epoch metrics\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    checkpoint_path = os.path.join(save_dir, f\"vit_epoch_{epoch + 1}.pth\")\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_loss': train_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'train_acc': train_acc,\n",
        "        'val_acc': val_acc,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {current_lr:.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf6G0luh4HdB",
        "outputId": "dacb0d61-7216-4105-daa9-fca2f4bd6d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training batches: 589, Validation batches: 148\n",
            "Epoch 1/10, Train Loss: 1.6836, Train Acc: 76.39%, Val Loss: 0.8562, Val Acc: 89.37%, LR: 5.00e-05\n",
            "Epoch 2/10, Train Loss: 1.0386, Train Acc: 80.57%, Val Loss: 0.5558, Val Acc: 89.66%, LR: 5.00e-05\n",
            "Epoch 3/10, Train Loss: 0.8185, Train Acc: 82.68%, Val Loss: 0.4614, Val Acc: 90.38%, LR: 5.00e-06\n",
            "Epoch 4/10, Train Loss: 0.6486, Train Acc: 86.17%, Val Loss: 0.4067, Val Acc: 91.51%, LR: 5.00e-06\n",
            "Epoch 5/10, Train Loss: 0.6001, Train Acc: 87.50%, Val Loss: 0.3934, Val Acc: 91.85%, LR: 5.00e-06\n",
            "Epoch 6/10, Train Loss: 0.5825, Train Acc: 87.59%, Val Loss: 0.3865, Val Acc: 92.10%, LR: 5.00e-07\n",
            "Epoch 7/10, Train Loss: 0.5607, Train Acc: 88.07%, Val Loss: 0.3843, Val Acc: 92.10%, LR: 5.00e-07\n",
            "Epoch 8/10, Train Loss: 0.5490, Train Acc: 88.21%, Val Loss: 0.3829, Val Acc: 92.04%, LR: 5.00e-07\n",
            "Epoch 9/10, Train Loss: 0.5579, Train Acc: 88.23%, Val Loss: 0.3821, Val Acc: 92.15%, LR: 5.00e-08\n",
            "Epoch 10/10, Train Loss: 0.5521, Train Acc: 88.33%, Val Loss: 0.3819, Val Acc: 92.15%, LR: 5.00e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"vit_labeled_model.pth\")"
      ],
      "metadata": {
        "id": "VUc-WkwsFXg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(\n",
        "        mean=feature_extractor.image_mean,  # Normalize using the mean of the pre-trained model\n",
        "        std=feature_extractor.image_std    # Normalize using the std of the pre-trained model\n",
        "    ),\n",
        "])\n",
        "# Custom Dataset for Validation Data\n",
        "class ValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "val_dir = \"/content/data/task1/val_data\"\n",
        "val_dataset = ValDataset(val_dir, transform=test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Validation dataset loaded: {len(val_dataset)} images.\")\n",
        "\n",
        "model.eval()\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.logits.max(1)  # Get predicted labels\n",
        "\n",
        "        # Append predictions with filenames\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": label})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Predictions saved to {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIfwk_0CELIq",
        "outputId": "edcce813-d608-4a75-e8af-39c3ce8d0d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation dataset loaded: 5000 images.\n",
            "Predictions saved to submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fixmatch VIT"
      ],
      "metadata": {
        "id": "2SnCBKuN65Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model name and number of labels\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"  # Replace with your model name if different\n",
        "num_labels = 100  # Adjust based on your dataset\n",
        "\n",
        "# Initialize the model\n",
        "model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "model.load_state_dict(torch.load(\"vit_labeled_model.pth\"))  # Replace with the path to your saved model\n",
        "model = model.to(device)\n",
        "\n",
        "print(\"ViT model trained on labeled data loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jo9hvDK67nL",
        "outputId": "b80944a4-0346-4f2b-8580-5a4bb94a5560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-18-df1b047870d8>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"vit_labeled_model.pth\"))  # Replace with the path to your saved model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT model trained on labeled data loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FixMatch loss\n",
        "def fixmatch_loss(weak_logits, strong_logits, threshold=0.95):\n",
        "    probs = F.softmax(weak_logits, dim=1)\n",
        "    pseudo_labels = torch.argmax(probs, dim=1)\n",
        "    mask = probs.max(dim=1)[0] >= threshold  # Confidence mask\n",
        "    if mask.sum() == 0:  # Handle cases with no confident pseudo-labels\n",
        "        return torch.tensor(0.0, requires_grad=True).to(weak_logits.device)\n",
        "    return F.cross_entropy(strong_logits[mask], pseudo_labels[mask])"
      ],
      "metadata": {
        "id": "oFdAC7XX8SB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extractor for normalization\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n",
        "\n",
        "# Define augmentations\n",
        "# Weak augmentation\n",
        "weak_augment = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# Strong augmentation\n",
        "strong_augment = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# Define datasets\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, annotations_df, image_dir, transform=None):\n",
        "        self.annotations_df = annotations_df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations_df.iloc[idx, 0]\n",
        "        label = self.annotations_df.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, weak_transform=None, strong_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.weak_transform = weak_transform\n",
        "        self.strong_transform = strong_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        weak_image = self.weak_transform(image) if self.weak_transform else image\n",
        "        strong_image = self.strong_transform(image) if self.strong_transform else image\n",
        "        return weak_image, strong_image, img_name\n",
        "\n",
        "# Load labeled and unlabeled datasets\n",
        "annotations_df = pd.read_csv(\"/content/data/task1/train_data/annotations.csv\")  # Adjust path as needed\n",
        "\n",
        "labeled_dataset = LabeledDataset(annotations_df, \"/content/data\", transform=weak_augment)\n",
        "unlabeled_dataset = UnlabeledDataset(\"/content/data/task1/train_data/images/unlabeled\", weak_transform=weak_augment, strong_transform=strong_augment)\n",
        "\n",
        "# Create dataloaders\n",
        "labeled_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STs8Djsr75if",
        "outputId": "6bdf5447-1839-4127-f982-9749e3947808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=5e-5, total_steps=num_epochs * len(labeled_loader)\n",
        ")\n",
        "# FixMatch training loop\n",
        "lambda_u_schedule = lambda epoch: min(1.0, epoch / 10)  # Gradually increase unsupervised weight\n",
        "\n",
        "initial_threshold = 0.6\n",
        "final_threshold = 0.95\n",
        "threshold_schedule = lambda epoch: initial_threshold + (final_threshold - initial_threshold) * (epoch / num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, supervised_loss, unsupervised_loss = 0, 0, 0\n",
        "    lambda_u = lambda_u_schedule(epoch)\n",
        "\n",
        "    for (labeled_batch, unlabeled_batch) in zip_longest(labeled_loader, unlabeled_loader, fillvalue=None):\n",
        "        if labeled_batch is None or unlabeled_batch is None:\n",
        "            continue\n",
        "\n",
        "        # Labeled data\n",
        "        labeled_imgs, labeled_labels = labeled_batch\n",
        "        labeled_imgs, labeled_labels = labeled_imgs.to(device), labeled_labels.to(device)\n",
        "        labeled_logits = model(labeled_imgs).logits\n",
        "        loss_supervised = F.cross_entropy(labeled_logits, labeled_labels)\n",
        "\n",
        "        # Unlabeled data\n",
        "        weak_imgs, strong_imgs, _ = unlabeled_batch\n",
        "        weak_imgs, strong_imgs = weak_imgs.to(device), strong_imgs.to(device)\n",
        "        weak_logits = model(weak_imgs).logits\n",
        "        strong_logits = model(strong_imgs).logits\n",
        "        loss_unsupervised = fixmatch_loss(weak_logits, strong_logits)\n",
        "\n",
        "        # Combine losses\n",
        "        loss = loss_supervised + lambda_u * loss_unsupervised\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        supervised_loss += loss_supervised.item()\n",
        "        unsupervised_loss += loss_unsupervised.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Lambda_u: {lambda_u:.2f}, \"\n",
        "          f\"Total Loss: {total_loss:.4f}, Supervised Loss: {supervised_loss:.4f}, Unsupervised Loss: {unsupervised_loss:.4f}\")\n",
        "\n",
        "     # Save checkpoint\n",
        "    torch.save(model.state_dict(), f\"vit_fixmatch_epoch_{epoch + 1}.pth\")\n",
        "    print(f\"Checkpoint saved for epoch {epoch + 1}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5Mb54WP8WDz",
        "outputId": "6f9a5ca8-cb5e-4016-dc81-fe7afcf30ece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Lambda_u: 0.00, Total Loss: 24.6367, Supervised Loss: 24.6367, Unsupervised Loss: 684.6248\n",
            "Checkpoint saved for epoch 1.\n",
            "Epoch 2/10, Lambda_u: 0.10, Total Loss: 82.3100, Supervised Loss: 18.5268, Unsupervised Loss: 637.8328\n",
            "Checkpoint saved for epoch 2.\n",
            "Epoch 3/10, Lambda_u: 0.20, Total Loss: 128.2911, Supervised Loss: 16.9113, Unsupervised Loss: 556.8992\n",
            "Checkpoint saved for epoch 3.\n",
            "Epoch 4/10, Lambda_u: 0.30, Total Loss: 175.5756, Supervised Loss: 15.6375, Unsupervised Loss: 533.1269\n",
            "Checkpoint saved for epoch 4.\n",
            "Epoch 5/10, Lambda_u: 0.40, Total Loss: 218.0119, Supervised Loss: 14.6839, Unsupervised Loss: 508.3201\n",
            "Checkpoint saved for epoch 5.\n",
            "Epoch 6/10, Lambda_u: 0.50, Total Loss: 268.5228, Supervised Loss: 13.9692, Unsupervised Loss: 509.1071\n",
            "Checkpoint saved for epoch 6.\n",
            "Epoch 7/10, Lambda_u: 0.60, Total Loss: 308.5190, Supervised Loss: 13.5064, Unsupervised Loss: 491.6876\n",
            "Checkpoint saved for epoch 7.\n",
            "Epoch 8/10, Lambda_u: 0.70, Total Loss: 349.2554, Supervised Loss: 12.9453, Unsupervised Loss: 480.4430\n",
            "Checkpoint saved for epoch 8.\n",
            "Epoch 9/10, Lambda_u: 0.80, Total Loss: 388.5643, Supervised Loss: 12.6177, Unsupervised Loss: 469.9333\n",
            "Checkpoint saved for epoch 9.\n",
            "Epoch 10/10, Lambda_u: 0.90, Total Loss: 423.9913, Supervised Loss: 12.4217, Unsupervised Loss: 457.2996\n",
            "Checkpoint saved for epoch 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vit91 + FixMatch"
      ],
      "metadata": {
        "id": "SoUVUau7NGcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model name and number of labels\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"  # Replace with your model name if different\n",
        "num_labels = 100  # Adjust based on your dataset\n",
        "\n",
        "# Initialize the model\n",
        "model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "model = model.to(device)  # Move model to device\n",
        "\n",
        "checkpoint_path = \"vit_epoch_10.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(\"Model loaded successfully from checkpoint!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jK1iM10iN5l-",
        "outputId": "4d87947a-6737-4553-e7f6-db47159adc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-7-143ffc5769cf>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from checkpoint!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FixMatch loss\n",
        "def fixmatch_loss(weak_logits, strong_logits, threshold=0.95):\n",
        "    probs = F.softmax(weak_logits, dim=1)\n",
        "    pseudo_labels = torch.argmax(probs, dim=1)\n",
        "    mask = probs.max(dim=1)[0] >= threshold  # Confidence mask\n",
        "    if mask.sum() == 0:  # Handle cases with no confident pseudo-labels\n",
        "        return torch.tensor(0.0, requires_grad=True).to(weak_logits.device)\n",
        "    return F.cross_entropy(strong_logits[mask], pseudo_labels[mask])\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    val_loss, correct, total = 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images).logits\n",
        "            val_loss += F.cross_entropy(outputs, labels, reduction='sum').item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "    val_loss /= total\n",
        "    val_accuracy = 100. * correct / total\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "pv6BYi1fPY5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature extractor for normalization\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "# Define augmentations\n",
        "# Weak augmentation\n",
        "weak_augment = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# Strong augmentation\n",
        "strong_augment = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# Define datasets\n",
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, annotations_df, image_dir, transform=None):\n",
        "        self.annotations_df = annotations_df\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations_df.iloc[idx, 0]\n",
        "        label = self.annotations_df.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, weak_transform=None, strong_transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.weak_transform = weak_transform\n",
        "        self.strong_transform = strong_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        weak_image = self.weak_transform(image) if self.weak_transform else image\n",
        "        strong_image = self.strong_transform(image) if self.strong_transform else image\n",
        "        return weak_image, strong_image, img_name\n",
        "\n",
        "# Load labeled and unlabeled datasets\n",
        "annotations_df = pd.read_csv(\"/content/data/task1/train_data/annotations.csv\")  # Adjust path as needed\n",
        "train_annotations, val_annotations = train_test_split(annotations_df, test_size=0.2, stratify=annotations_df['label'])\n",
        "\n",
        "train_dataset = LabeledDataset(train_annotations, \"/content/data\", transform=weak_augment)\n",
        "val_dataset = LabeledDataset(val_annotations, \"/content/data\", transform=weak_augment)\n",
        "\n",
        "unlabeled_dataset = UnlabeledDataset(\"/content/data/task1/train_data/images/unlabeled\", weak_transform=weak_augment, strong_transform=strong_augment)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1d4aee4a0df2481ca910237b0272ecb7",
            "601baf2be6074c3fb864689b2a827da5",
            "35a01a9700b948e4ae198aab86e34246",
            "40462397ba754771bad03f450f1fe09a",
            "83ade269cec642b0a71024c4426d8d74",
            "cdc6196d02b7472b85f109e60594112a",
            "b2aba34509d3426a91461fa5e06dfc8b",
            "8713ff909b2145f7a93f03c7c4a4f5f0",
            "99c91ff5f1ec4983aab45a2f163faaba",
            "99a35e887a8e4a9d8de89bb86326c01e",
            "4ab743f11fcd4a7c9b310713acf31991"
          ]
        },
        "id": "_ONI2Im-QNYS",
        "outputId": "9d6584c7-a6b4-4f78-cc89-184c5cdd98cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d4aee4a0df2481ca910237b0272ecb7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-6)  # Use a lower learning rate for fine-tuning\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# FixMatch training loop\n",
        "lambda_u_schedule = lambda epoch: min(1.0, epoch / 10)  # Gradually increase unsupervised weight\n",
        "\n",
        "initial_threshold = 0.6\n",
        "final_threshold = 0.95\n",
        "threshold_schedule = lambda epoch: initial_threshold + (final_threshold - initial_threshold) * (epoch / num_epochs)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss, supervised_loss, unsupervised_loss, train_correct, train_total = 0, 0, 0, 0, 0\n",
        "    lambda_u = lambda_u_schedule(epoch)\n",
        "    threshold = threshold_schedule(epoch)\n",
        "\n",
        "    for (labeled_batch, unlabeled_batch) in zip_longest(train_loader, unlabeled_loader, fillvalue=None):\n",
        "        if labeled_batch is None or unlabeled_batch is None:\n",
        "            continue\n",
        "\n",
        "        # Labeled data\n",
        "        labeled_imgs, labeled_labels = labeled_batch\n",
        "        labeled_imgs, labeled_labels = labeled_imgs.to(device), labeled_labels.to(device)\n",
        "        labeled_logits = model(labeled_imgs).logits\n",
        "        loss_supervised = F.cross_entropy(labeled_logits, labeled_labels)\n",
        "\n",
        "        # Compute train accuracy\n",
        "        train_correct += (labeled_logits.argmax(dim=1) == labeled_labels).sum().item()\n",
        "        train_total += labeled_labels.size(0)\n",
        "\n",
        "        # Unlabeled data\n",
        "        weak_imgs, strong_imgs, _ = unlabeled_batch\n",
        "        weak_imgs, strong_imgs = weak_imgs.to(device), strong_imgs.to(device)\n",
        "        weak_logits = model(weak_imgs).logits\n",
        "        strong_logits = model(strong_imgs).logits\n",
        "        loss_unsupervised = fixmatch_loss(weak_logits, strong_logits, threshold=threshold)\n",
        "\n",
        "        # Combine losses\n",
        "        loss = loss_supervised + lambda_u * loss_unsupervised\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        supervised_loss += loss_supervised.item()\n",
        "        unsupervised_loss += loss_unsupervised.item()\n",
        "\n",
        "    # Normalize losses\n",
        "    avg_total_loss = total_loss / len(train_loader)\n",
        "    avg_supervised_loss = supervised_loss / len(train_loader)\n",
        "    avg_unsupervised_loss = unsupervised_loss / len(train_loader)\n",
        "\n",
        "    # Compute training accuracy\n",
        "    train_accuracy = 100. * train_correct / train_total\n",
        "\n",
        "    val_loss, val_accuracy = evaluate(model, val_loader)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = scheduler.get_last_lr()[0]\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, LR: {current_lr:.6f}, Lambda_u: {lambda_u:.2f}, Threshold: {threshold:.2f}, \"\n",
        "          f\"Train Loss: {avg_total_loss:.4f}, Supervised Loss: {avg_supervised_loss:.4f}, \"\n",
        "          f\"Unsupervised Loss: {avg_unsupervised_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch + 1,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_loss': avg_total_loss,\n",
        "        'val_loss': val_loss,\n",
        "        'train_accuracy': train_accuracy,\n",
        "        'val_accuracy': val_accuracy,\n",
        "    }, f\"vit_fixmatch_epoch_{epoch + 1}.pth\")\n",
        "    print(f\"Checkpoint saved for epoch {epoch + 1}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWjhUyHRRzhI",
        "outputId": "f2f3446d-30f6-43fe-8508-b950306338c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, LR: 0.000005, Lambda_u: 0.00, Threshold: 0.60, Train Loss: 0.1971, Supervised Loss: 0.1971, Unsupervised Loss: 1.0634, Train Accuracy: 96.66%, Val Loss: 0.1808, Val Accuracy: 96.96%\n",
            "Checkpoint saved for epoch 1.\n",
            "Epoch 2/10, LR: 0.000005, Lambda_u: 0.10, Threshold: 0.64, Train Loss: 0.2432, Supervised Loss: 0.1395, Unsupervised Loss: 1.0375, Train Accuracy: 97.96%, Val Loss: 0.1742, Val Accuracy: 96.69%\n",
            "Checkpoint saved for epoch 2.\n",
            "Epoch 3/10, LR: 0.000004, Lambda_u: 0.20, Threshold: 0.67, Train Loss: 0.2989, Supervised Loss: 0.1108, Unsupervised Loss: 0.9408, Train Accuracy: 98.57%, Val Loss: 0.1669, Val Accuracy: 96.82%\n",
            "Checkpoint saved for epoch 3.\n",
            "Epoch 4/10, LR: 0.000003, Lambda_u: 0.30, Threshold: 0.70, Train Loss: 0.3523, Supervised Loss: 0.0915, Unsupervised Loss: 0.8692, Train Accuracy: 98.97%, Val Loss: 0.1614, Val Accuracy: 96.77%\n",
            "Checkpoint saved for epoch 4.\n",
            "Epoch 5/10, LR: 0.000003, Lambda_u: 0.40, Threshold: 0.74, Train Loss: 0.4050, Supervised Loss: 0.0778, Unsupervised Loss: 0.8180, Train Accuracy: 99.23%, Val Loss: 0.1597, Val Accuracy: 96.82%\n",
            "Checkpoint saved for epoch 5.\n",
            "Epoch 6/10, LR: 0.000002, Lambda_u: 0.50, Threshold: 0.77, Train Loss: 0.4716, Supervised Loss: 0.0713, Unsupervised Loss: 0.8006, Train Accuracy: 99.31%, Val Loss: 0.1552, Val Accuracy: 96.96%\n",
            "Checkpoint saved for epoch 6.\n",
            "Epoch 7/10, LR: 0.000001, Lambda_u: 0.60, Threshold: 0.81, Train Loss: 0.5360, Supervised Loss: 0.0656, Unsupervised Loss: 0.7840, Train Accuracy: 99.43%, Val Loss: 0.1549, Val Accuracy: 96.77%\n",
            "Checkpoint saved for epoch 7.\n",
            "Epoch 8/10, LR: 0.000000, Lambda_u: 0.70, Threshold: 0.84, Train Loss: 0.5852, Supervised Loss: 0.0625, Unsupervised Loss: 0.7466, Train Accuracy: 99.45%, Val Loss: 0.1549, Val Accuracy: 96.67%\n",
            "Checkpoint saved for epoch 8.\n",
            "Epoch 9/10, LR: 0.000000, Lambda_u: 0.80, Threshold: 0.88, Train Loss: 0.6317, Supervised Loss: 0.0611, Unsupervised Loss: 0.7133, Train Accuracy: 99.50%, Val Loss: 0.1528, Val Accuracy: 96.88%\n",
            "Checkpoint saved for epoch 9.\n",
            "Epoch 10/10, LR: 0.000000, Lambda_u: 0.90, Threshold: 0.92, Train Loss: 0.6711, Supervised Loss: 0.0609, Unsupervised Loss: 0.6780, Train Accuracy: 99.49%, Val Loss: 0.1536, Val Accuracy: 96.88%\n",
            "Checkpoint saved for epoch 10.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(\n",
        "        mean=feature_extractor.image_mean,  # Normalize using the mean of the pre-trained model\n",
        "        std=feature_extractor.image_std    # Normalize using the std of the pre-trained model\n",
        "    ),\n",
        "])\n",
        "# Custom Dataset for Validation Data\n",
        "class ValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "val_dir = \"/content/data/task1/val_data\"\n",
        "val_dataset = ValDataset(val_dir, transform=test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Validation dataset loaded: {len(val_dataset)} images.\")\n",
        "\n",
        "model.eval()\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in val_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.logits.max(1)  # Get predicted labels\n",
        "\n",
        "        # Append predictions with filenames\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": label})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Predictions saved to {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLcwtZFOxSCE",
        "outputId": "66a0d793-08a1-4523-843d-5b84c33c046c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation dataset loaded: 5000 images.\n",
            "Predictions saved to submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble"
      ],
      "metadata": {
        "id": "q_wV-Q3Q0IRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to models and test dataset\n",
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model name and number of labels\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"  # Replace with your model name if different\n",
        "num_labels = 100  # Adjust based on your dataset\n",
        "\n",
        "# Load Fine-Tuned Model\n",
        "model_ft = ViTForImageClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "model_ft = model_ft.to(device)  # Move model to device\n",
        "\n",
        "checkpoint_ft_path = \"vit_epoch_10.pth\"\n",
        "checkpoint_ft = torch.load(checkpoint_ft_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_ft.load_state_dict(checkpoint_ft['model_state_dict'])\n",
        "\n",
        "print(\"Model ft loaded successfully from checkpoint!\")\n",
        "\n",
        "\n",
        "# Load FixMatch Model\n",
        "model_fix = ViTForImageClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "model_fix = model_fix.to(device)  # Move model to device\n",
        "\n",
        "checkpoint_fix_path = \"vit91_fixmatch_epoch_10.pth\"\n",
        "checkpoint_fix = torch.load(checkpoint_fix_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "model_fix.load_state_dict(checkpoint_fix['model_state_dict'])\n",
        "\n",
        "print(\"Model ft loaded successfully from checkpoint!\")\n",
        "\n",
        "model_ft.eval()\n",
        "model_fix.eval()\n",
        "\n",
        "# Feature extractor for normalization\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "# Custom Dataset for Validation Data\n",
        "class ValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "val_dir = \"/content/data/task1/val_data\"\n",
        "val_dataset = ValDataset(val_dir, transform=test_transform)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "# Ensemble Predictions\n",
        "predictions = []\n",
        "weight_ft = 0.4  # Weight for fine-tuned model\n",
        "weight_fix = 0.6  # Weight for FixMatch model\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in val_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Generate predictions from both models\n",
        "        logits_fine_tuned = model_ft(images).logits\n",
        "        logits_fixmatch = model_fix(images).logits\n",
        "\n",
        "        # Average probabilities (ensemble method)\n",
        "        probs_fine_tuned = torch.softmax(logits_fine_tuned, dim=1)\n",
        "        probs_fixmatch = torch.softmax(logits_fixmatch, dim=1)\n",
        "        avg_probs = (weight_ft * probs_fine_tuned + weight_fix * probs_fixmatch)\n",
        "\n",
        "        # Predicted classes\n",
        "        _, predicted = avg_probs.max(1)\n",
        "\n",
        "        # Append predictions\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": label})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"ensemble_submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Ensemble predictions saved to {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBrrKEkX0HbL",
        "outputId": "28a31abe-b420-4d13-bc1f-a68e8bb6e269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-5-2641e93e7723>:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_ft = torch.load(checkpoint_ft_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ft loaded successfully from checkpoint!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-5-2641e93e7723>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_fix = torch.load(checkpoint_fix_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model ft loaded successfully from checkpoint!\n",
            "Ensemble predictions saved to ensemble_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_ft = []\n",
        "predictions_fix = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in val_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Generate predictions\n",
        "        logits_ft = model_ft(images).logits\n",
        "        logits_fix = model_fix(images).logits\n",
        "\n",
        "        preds_ft = logits_ft.argmax(dim=1).cpu().numpy()\n",
        "        preds_fix = logits_fix.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "        predictions_ft.extend(preds_ft)\n",
        "        predictions_fix.extend(preds_fix)\n",
        "\n",
        "overlap = sum([1 for ft, fix in zip(predictions_ft, predictions_fix) if ft == fix])\n",
        "print(f\"Overlap: {overlap}/{len(predictions_ft)} ({100 * overlap / len(predictions_ft):.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYeuCK2u4N1c",
        "outputId": "1194d917-069b-4b6b-acc4-1aca06fdb7c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overlap: 4818/5000 (96.36%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disagree_indices = [i for i, (ft, fix) in enumerate(zip(predictions_ft, predictions_fix)) if ft != fix]\n",
        "print(f\"Number of disagreements: {len(disagree_indices)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgb7JkSQ4rTZ",
        "outputId": "b70a01ed-baf3-4251-a4b5-5351e9d43663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of disagreements: 182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble Predictions with Confidence-Based Fallback\n",
        "predictions = []\n",
        "weight_ft = 0.7  # Weight for fine-tuned model\n",
        "weight_fix = 0.3  # Weight for FixMatch model\n",
        "confidence_threshold = 0.7  # Confidence threshold for fallback strategy\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in tqdm(val_loader, desc=\"Ensembling Predictions\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Generate predictions from both models\n",
        "        logits_fine_tuned = model_ft(images).logits\n",
        "        logits_fixmatch = model_fix(images).logits\n",
        "\n",
        "        # Calculate probabilities\n",
        "        probs_fine_tuned = torch.softmax(logits_fine_tuned, dim=1)\n",
        "        probs_fixmatch = torch.softmax(logits_fixmatch, dim=1)\n",
        "\n",
        "        # Weighted average probabilities\n",
        "        avg_probs = (weight_ft * probs_fine_tuned + weight_fix * probs_fixmatch)\n",
        "\n",
        "        for img_name, prob_ft, prob_fix, avg_prob in zip(img_names, probs_fine_tuned, probs_fixmatch, avg_probs):\n",
        "            # Confidence-based fallback for disagreement cases\n",
        "            if prob_ft.argmax() != prob_fix.argmax():\n",
        "                avg_confidence = ((weight_ft * prob_ft.max()) + (weight_fix * prob_fix.max())) / (weight_ft + weight_fix)\n",
        "                if avg_confidence < confidence_threshold:\n",
        "                    if prob_ft.max() > prob_fix.max():\n",
        "                        final_prediction = prob_ft.argmax()  # Fine-tuned model\n",
        "                    else:\n",
        "                        final_prediction = prob_fix.argmax()  # FixMatch model\n",
        "                else:\n",
        "                    final_prediction = avg_prob.argmax()  # Average\n",
        "            else:\n",
        "                final_prediction = avg_prob.argmax()  # Agreement case\n",
        "\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": final_prediction.item()})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"ensemble_confidence_submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Ensemble predictions saved to {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQMc2NB35FU1",
        "outputId": "adf15ec6-2cdc-49fb-a2f8-beefe7d8cc22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ensembling Predictions: 100%|██████████| 157/157 [00:58<00:00,  2.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble predictions saved to ensemble_confidence_submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pseudo labeling"
      ],
      "metadata": {
        "id": "TFuU_ndahept"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model name and number of labels\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"  # Replace with your model name if different\n",
        "num_labels = 100  # Adjust based on your dataset\n",
        "\n",
        "# Initialize the model\n",
        "model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "model = model.to(device)  # Move model to device\n",
        "\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/AAIT/HW2/task1/vit_epoch_10.pth\"\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "print(\"Model loaded successfully from checkpoint!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EE76rn8Bhipf",
        "outputId": "68cd7ab6-da66-4cc6-d301-81c2ec068b2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-19-c8ab83536ce3>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from checkpoint!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "  transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "  transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
        "  transforms.Normalize(\n",
        "      mean=feature_extractor.image_mean,  # Normalize using the mean of the pre-trained model\n",
        "      std=feature_extractor.image_std    # Normalize using the std of the pre-trained model\n",
        "  ),\n",
        "])\n",
        "\n",
        " # Custom Dataset for Validation Data\n",
        "class UnlabeledDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "unlabeled_dir = \"/content/data/task1/train_data/images/unlabeled\"\n",
        "unlabeled_dataset = UnlabeledDataset(unlabeled_dir, transform=test_transform)\n",
        "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Unlabeled dataset loaded: {len(unlabeled_dataset)} images.\")"
      ],
      "metadata": {
        "id": "EEvwPLUJkNXc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134e094f-c5cd-466c-a76b-ed5665bdcbfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unlabeled dataset loaded: 26445 images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define confidence threshold\n",
        "confidence_threshold = 0.90\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# List to store pseudo-labeled data\n",
        "pseudo_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in tqdm(unlabeled_loader):\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Extract logits and compute probabilities\n",
        "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "        max_probs, predicted = probabilities.max(1)  # Get max probability and predicted class\n",
        "\n",
        "        # Filter predictions by confidence threshold\n",
        "        for img_name, label, prob in zip(img_names, predicted.cpu().numpy(), max_probs.cpu().numpy()):\n",
        "            if prob >= confidence_threshold:\n",
        "                pseudo_labels.append({\"image_name\": img_name, \"label\": label, \"confidence\": prob})\n",
        "\n",
        "pseudo_labels_df = pd.DataFrame(pseudo_labels)\n",
        "pseudo_labels_df.to_csv(\"pseudo_labels.csv\", index=False)\n",
        "print(f\"Generated {len(pseudo_labels)} pseudo-labeled samples.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwLc9Y_Uj_p2",
        "outputId": "5e2dc3d3-a04e-4044-ce02-80940c1d93b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 827/827 [01:39<00:00,  8.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 17719 pseudo-labeled samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_file = \"/content/data/task1/train_data/annotations.csv\"\n",
        "\n",
        "annotations = pd.read_csv(annotations_file)\n",
        "\n",
        "print(f\"Total number of entries in the annotations file: {len(annotations)}\")\n",
        "annotations['sample'] = \"/content/data/\" + annotations['sample']\n",
        "annotations.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "ODYrMN51sn90",
        "outputId": "cda74ac9-4621-4be2-df37-d13dc7289aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of entries in the annotations file: 23555\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              sample  label\n",
              "0  /content/data/task1/train_data/images/labeled/...      0\n",
              "1  /content/data/task1/train_data/images/labeled/...      1\n",
              "2  /content/data/task1/train_data/images/labeled/...      2\n",
              "3  /content/data/task1/train_data/images/labeled/...      3\n",
              "4  /content/data/task1/train_data/images/labeled/...      4"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74c96b35-7a28-48cb-b2d9-d90c28fc4958\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74c96b35-7a28-48cb-b2d9-d90c28fc4958')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-74c96b35-7a28-48cb-b2d9-d90c28fc4958 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-74c96b35-7a28-48cb-b2d9-d90c28fc4958');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ae5a83b0-a3e7-4951-b070-7c3adc997f0e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ae5a83b0-a3e7-4951-b070-7c3adc997f0e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ae5a83b0-a3e7-4951-b070-7c3adc997f0e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations",
              "summary": "{\n  \"name\": \"annotations\",\n  \"rows\": 23555,\n  \"fields\": [\n    {\n      \"column\": \"sample\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 23555,\n        \"samples\": [\n          \"/content/data/task1/train_data/images/labeled/4934.jpeg\",\n          \"/content/data/task1/train_data/images/labeled/4013.jpeg\",\n          \"/content/data/task1/train_data/images/labeled/1056.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pseudo_labels_df = pd.DataFrame(pseudo_labels)\n",
        "pseudo_labels_df.to_csv(\"pseudo_labels.csv\", index=False)\n",
        "print(f\"Generated {len(pseudo_labels)} pseudo-labeled samples.\")\n",
        "pseudo_labels_df['image_name'] = \"/content/data/task1/train_data/images/unlabeled/\" + pseudo_labels_df['image_name']\n",
        "pseudo_labels_df.rename(columns={'image_name': 'sample'}, inplace=True)\n",
        "pseudo_labels_df = pseudo_labels_df[['sample', 'label']]\n",
        "pseudo_labels_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "LywMr7NFsLrw",
        "outputId": "b5e742bd-6feb-4d59-97fd-4ced31aba6a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 17719 pseudo-labeled samples.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  sample  label\n",
              "0      /content/data/task1/train_data/images/unlabele...     28\n",
              "1      /content/data/task1/train_data/images/unlabele...     28\n",
              "2      /content/data/task1/train_data/images/unlabele...     87\n",
              "3      /content/data/task1/train_data/images/unlabele...     11\n",
              "4      /content/data/task1/train_data/images/unlabele...     48\n",
              "...                                                  ...    ...\n",
              "17714  /content/data/task1/train_data/images/unlabele...      6\n",
              "17715  /content/data/task1/train_data/images/unlabele...     57\n",
              "17716  /content/data/task1/train_data/images/unlabele...     47\n",
              "17717  /content/data/task1/train_data/images/unlabele...      3\n",
              "17718  /content/data/task1/train_data/images/unlabele...     45\n",
              "\n",
              "[17719 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9a166b6-e1aa-44cb-8f09-c74aeec89951\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>87</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17714</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17715</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17716</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17717</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17718</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17719 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9a166b6-e1aa-44cb-8f09-c74aeec89951')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9a166b6-e1aa-44cb-8f09-c74aeec89951 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9a166b6-e1aa-44cb-8f09-c74aeec89951');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0435900a-d05c-4ead-bced-f21d55f1a5c8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0435900a-d05c-4ead-bced-f21d55f1a5c8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0435900a-d05c-4ead-bced-f21d55f1a5c8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a03d8efe-d994-438e-b751-82fe71d92a37\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('pseudo_labels_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a03d8efe-d994-438e-b751-82fe71d92a37 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('pseudo_labels_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "pseudo_labels_df",
              "summary": "{\n  \"name\": \"pseudo_labels_df\",\n  \"rows\": 17719,\n  \"fields\": [\n    {\n      \"column\": \"sample\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17719,\n        \"samples\": [\n          \"/content/data/task1/train_data/images/unlabeled/18966.jpeg\",\n          \"/content/data/task1/train_data/images/unlabeled/7714.jpeg\",\n          \"/content/data/task1/train_data/images/unlabeled/2668.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 95,\n        \"samples\": [\n          16,\n          15,\n          19\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([annotations, pseudo_labels_df], ignore_index=True)\n",
        "combined_df.to_csv(\"combined_dataset.csv\", index=False)\n",
        "combined_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "D_znf7_ev4-5",
        "outputId": "84d283d8-08e8-4594-bb07-3bce2cdcaebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  sample  label\n",
              "0      /content/data/task1/train_data/images/labeled/...      0\n",
              "1      /content/data/task1/train_data/images/labeled/...      1\n",
              "2      /content/data/task1/train_data/images/labeled/...      2\n",
              "3      /content/data/task1/train_data/images/labeled/...      3\n",
              "4      /content/data/task1/train_data/images/labeled/...      4\n",
              "...                                                  ...    ...\n",
              "41269  /content/data/task1/train_data/images/unlabele...      6\n",
              "41270  /content/data/task1/train_data/images/unlabele...     57\n",
              "41271  /content/data/task1/train_data/images/unlabele...     47\n",
              "41272  /content/data/task1/train_data/images/unlabele...      3\n",
              "41273  /content/data/task1/train_data/images/unlabele...     45\n",
              "\n",
              "[41274 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-03abd279-1987-4a9e-9738-1f8a0d944eb6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sample</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/data/task1/train_data/images/labeled/...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41269</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41270</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41271</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41272</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41273</th>\n",
              "      <td>/content/data/task1/train_data/images/unlabele...</td>\n",
              "      <td>45</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41274 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-03abd279-1987-4a9e-9738-1f8a0d944eb6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-03abd279-1987-4a9e-9738-1f8a0d944eb6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-03abd279-1987-4a9e-9738-1f8a0d944eb6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-63abfbf8-9013-4fdb-b6ac-41d089422be7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-63abfbf8-9013-4fdb-b6ac-41d089422be7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-63abfbf8-9013-4fdb-b6ac-41d089422be7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f4e72cb9-adc4-45df-94c2-f1c5e5d87e36\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('combined_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f4e72cb9-adc4-45df-94c2-f1c5e5d87e36 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('combined_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "combined_df",
              "summary": "{\n  \"name\": \"combined_df\",\n  \"rows\": 41274,\n  \"fields\": [\n    {\n      \"column\": \"sample\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 41274,\n        \"samples\": [\n          \"/content/data/task1/train_data/images/labeled/532.jpeg\",\n          \"/content/data/task1/train_data/images/unlabeled/26086.jpeg\",\n          \"/content/data/task1/train_data/images/labeled/22110.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, annotations_df, transform=None):\n",
        "        self.annotations_df = annotations_df\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.annotations_df.iloc[idx, 0]\n",
        "        label = self.annotations_df.iloc[idx, 1]\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "])\n",
        "\n",
        "\n",
        "combined_file = \"/content/combined_dataset.csv\"\n",
        "\n",
        "combined = pd.read_csv(combined_file)\n",
        "\n",
        "train_annotations, val_annotations = train_test_split(combined, test_size=0.2, stratify=combined['label'], random_state=42)\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset(train_annotations, transform=train_transform)\n",
        "val_dataset = CustomDataset(val_annotations, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-6)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.logits.max(1)  # Access logits\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * correct / total\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.logits, labels)  # Use logits for validation\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.logits.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "\n",
        "    # Print epoch metrics\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    torch.save(model.state_dict(), f\"vit_pseudo_epoch_{epoch + 1}.pth\")\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {current_lr:.2e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2EANRxTwPjm",
        "outputId": "a83850e4-643d-4a72-af53-8961a602eb9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1032\n",
            "258\n",
            "Epoch 1/5, Train Loss: 0.7041, Train Acc: 84.50%, Val Loss: 0.1437, Val Acc: 97.92%, LR: 1.00e-06\n",
            "Epoch 2/5, Train Loss: 0.6810, Train Acc: 85.01%, Val Loss: 0.1433, Val Acc: 97.89%, LR: 1.00e-06\n",
            "Epoch 3/5, Train Loss: 0.6569, Train Acc: 85.62%, Val Loss: 0.1421, Val Acc: 97.82%, LR: 1.00e-06\n",
            "Epoch 4/5, Train Loss: 0.6529, Train Acc: 85.51%, Val Loss: 0.1416, Val Acc: 97.77%, LR: 1.00e-06\n",
            "Epoch 5/5, Train Loss: 0.6279, Train Acc: 86.24%, Val Loss: 0.1403, Val Acc: 97.86%, LR: 1.00e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust optimizer and scheduler\n",
        "optimizer = optim.AdamW(model.parameters(), lr=3e-6)  # Slightly higher learning rate\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "combined_file = \"/content/combined_dataset.csv\"\n",
        "\n",
        "combined = pd.read_csv(combined_file)\n",
        "\n",
        "train_annotations, val_annotations = train_test_split(combined, test_size=0.2, stratify=combined['label'], random_state=42)\n",
        "\n",
        "\n",
        "train_dataset = CustomDataset(train_annotations, transform=train_transform)\n",
        "val_dataset = CustomDataset(val_annotations, transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "\n",
        "\n",
        "# Continue training\n",
        "num_additional_epochs = 10\n",
        "for epoch in range(num_additional_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.logits.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = 100.0 * correct / total\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = outputs.logits.max(1)\n",
        "            val_total += labels.size(0)\n",
        "            val_correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = 100.0 * val_correct / val_total\n",
        "\n",
        "    # Adjust learning rate\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save model\n",
        "    torch.save(model.state_dict(), f\"vit_continued_epoch_{epoch + 6}.pth\")\n",
        "\n",
        "    # Print epoch metrics\n",
        "    print(f\"Epoch {epoch + 6}/{num_additional_epochs + 5}, \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, LR: {scheduler.get_last_lr()[0]:.2e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlNDfKzpdpIh",
        "outputId": "d725fd65-c682-4bb7-afed-4da4a009cba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1032\n",
            "258\n",
            "Epoch 6/15, Train Loss: 0.6077, Train Acc: 86.65%, Val Loss: 0.1352, Val Acc: 97.80%, LR: 2.93e-06\n",
            "Epoch 7/15, Train Loss: 0.5851, Train Acc: 86.99%, Val Loss: 0.1321, Val Acc: 97.82%, LR: 2.71e-06\n",
            "Epoch 8/15, Train Loss: 0.5708, Train Acc: 87.08%, Val Loss: 0.1283, Val Acc: 97.72%, LR: 2.38e-06\n",
            "Epoch 9/15, Train Loss: 0.5644, Train Acc: 87.38%, Val Loss: 0.1259, Val Acc: 97.80%, LR: 1.96e-06\n",
            "Epoch 10/15, Train Loss: 0.5639, Train Acc: 87.36%, Val Loss: 0.1250, Val Acc: 97.83%, LR: 1.50e-06\n",
            "Epoch 11/15, Train Loss: 0.5522, Train Acc: 87.51%, Val Loss: 0.1231, Val Acc: 97.86%, LR: 1.04e-06\n",
            "Epoch 12/15, Train Loss: 0.5399, Train Acc: 87.65%, Val Loss: 0.1225, Val Acc: 97.78%, LR: 6.18e-07\n",
            "Epoch 13/15, Train Loss: 0.5344, Train Acc: 87.79%, Val Loss: 0.1215, Val Acc: 97.83%, LR: 2.86e-07\n",
            "Epoch 14/15, Train Loss: 0.5404, Train Acc: 87.74%, Val Loss: 0.1212, Val Acc: 97.83%, LR: 7.34e-08\n",
            "Epoch 15/15, Train Loss: 0.5314, Train Acc: 88.02%, Val Loss: 0.1211, Val Acc: 97.83%, LR: 0.00e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define test transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(\n",
        "        mean=feature_extractor.image_mean,  # Normalize using the mean of the pre-trained model\n",
        "        std=feature_extractor.image_std    # Normalize using the std of the pre-trained model\n",
        "    ),\n",
        "])\n",
        "# Custom Dataset for Validation Data\n",
        "class ValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "test_dir = \"/content/data/task1/val_data\"\n",
        "test_dataset = ValDataset(test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Validation dataset loaded: {len(test_dataset)} images.\")\n",
        "\n",
        "model.eval()\n",
        "# Generate predictions\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.logits.max(1)  # Get predicted labels\n",
        "\n",
        "        # Append predictions with filenames\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": label})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"pseudo_submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"Predictions saved to {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps4KVMqKBcd3",
        "outputId": "be8bdfd6-10e2-4a5d-fbe1-0e8809df2d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation dataset loaded: 5000 images.\n",
            "Predictions saved to pseudo_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TTA"
      ],
      "metadata": {
        "id": "tJ4WUCs34OM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTForImageClassification, ViTImageProcessor\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model name and number of labels\n",
        "model_name = \"google/vit-base-patch16-224-in21k\"  # Replace with your model name if different\n",
        "num_labels = 100  # Adjust based on your dataset\n",
        "\n",
        "# Initialize the model\n",
        "model = ViTForImageClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "model = model.to(device)  # Move model to device\n",
        "\n",
        "feature_extractor = ViTImageProcessor.from_pretrained(model_name)\n",
        "\n",
        "checkpoint_path = \"/content/vit_continued_epoch_15.pth\"\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.load_state_dict(torch.load(checkpoint_path))  # Load checkpoint\n",
        "\n",
        "print(\"Model loaded successfully from checkpoint!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKKlZPh54P03",
        "outputId": "0eebc7f4-ad17-4cae-9b2d-f7781b3714fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-6-d9e1bc2e2ef3>:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=device)  # Ensure checkpoint is loaded to the correct device\n",
            "<ipython-input-6-d9e1bc2e2ef3>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(checkpoint_path))  # Load checkpoint\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from checkpoint!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "# Define test transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),         # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(\n",
        "        mean=feature_extractor.image_mean,  # Normalize using the mean of the pre-trained model\n",
        "        std=feature_extractor.image_std    # Normalize using the std of the pre-trained model\n",
        "    ),\n",
        "])\n",
        "# Custom Dataset for Validation Data\n",
        "class ValDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_files = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, img_name\n",
        "\n",
        "# Create validation dataset and dataloader\n",
        "test_dir = \"/content/data/task1/val_data\"\n",
        "test_dataset = ValDataset(test_dir, transform=test_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"Validation dataset loaded: {len(test_dataset)} images.\")\n",
        "\n",
        "\n",
        "tta_transforms = [\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "    ]),\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(p=1.0),  # Horizontal Flip\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "    ]),\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomRotation(15),  # Small rotation\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
        "    ]),\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        tta_predictions = []\n",
        "        for tta_transform in tta_transforms:\n",
        "            # Apply transforms to each image in the batch\n",
        "            augmented_images = torch.stack(\n",
        "                [tta_transform(to_pil_image(image.cpu())) for image in images]\n",
        "            ).to(device)\n",
        "            outputs = model(augmented_images)\n",
        "            tta_predictions.append(outputs.logits.softmax(dim=1))  # Softmax for probabilities\n",
        "\n",
        "        # Average predictions across TTA transforms\n",
        "        avg_predictions = torch.mean(torch.stack(tta_predictions), dim=0)\n",
        "        predicted_labels = avg_predictions.argmax(dim=1)\n",
        "\n",
        "        # Append predictions with filenames\n",
        "        for img_name, label in zip(img_names, predicted_labels.cpu().numpy()):\n",
        "            predictions.append({\"ID\": f\"task1/val_data/{img_name}\", \"label\": label})\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "submission_file = \"tta_submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "print(f\"TTA predictions saved to {submission_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYbgpUq35JEY",
        "outputId": "29bc127b-4067-4b94-e879-5196d8caac44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation dataset loaded: 5000 images.\n",
            "TTA predictions saved to tta_submission.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2"
      ],
      "metadata": {
        "id": "hj6pda5j3NoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = \"/content/aait-asg-2-task-2.zip\"\n",
        "extract_path = \"/content/data\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(len(os.listdir('/content/data/task2/train_data/images/labeled')))\n",
        "print(len(os.listdir('/content/data/task2/val_data')))\n",
        "\n",
        "annotations_file = \"/content/data/task2/train_data/annotations.csv\"\n",
        "\n",
        "annotations = pd.read_csv(annotations_file)\n",
        "\n",
        "print(f\"Total number of entries in the annotations file: {len(annotations)}\")\n",
        "\n",
        "annotations.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "id": "TLAlNOgD3NKm",
        "outputId": "848a526a-8dac-485f-cb01-c7386d78c11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "5000\n",
            "Total number of entries in the annotations file: 50000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             renamed_path  label_idx\n",
              "0  task2/train_data/images/labeled/0.jpeg          0\n",
              "1  task2/train_data/images/labeled/1.jpeg          0\n",
              "2  task2/train_data/images/labeled/2.jpeg          0\n",
              "3  task2/train_data/images/labeled/3.jpeg          0\n",
              "4  task2/train_data/images/labeled/4.jpeg          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1b7a9879-c535-4f94-9815-bb4a5591d10b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>renamed_path</th>\n",
              "      <th>label_idx</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>task2/train_data/images/labeled/0.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>task2/train_data/images/labeled/1.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>task2/train_data/images/labeled/2.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>task2/train_data/images/labeled/3.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>task2/train_data/images/labeled/4.jpeg</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1b7a9879-c535-4f94-9815-bb4a5591d10b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1b7a9879-c535-4f94-9815-bb4a5591d10b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1b7a9879-c535-4f94-9815-bb4a5591d10b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0a8eba9-bb48-4c11-8739-aaf541bb85e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0a8eba9-bb48-4c11-8739-aaf541bb85e9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0a8eba9-bb48-4c11-8739-aaf541bb85e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "annotations",
              "summary": "{\n  \"name\": \"annotations\",\n  \"rows\": 50000,\n  \"fields\": [\n    {\n      \"column\": \"renamed_path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 50000,\n        \"samples\": [\n          \"task2/train_data/images/labeled/33553.jpeg\",\n          \"task2/train_data/images/labeled/9427.jpeg\",\n          \"task2/train_data/images/labeled/199.jpeg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label_idx\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 29,\n        \"min\": 0,\n        \"max\": 99,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          83,\n          53,\n          70\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_counts = annotations['label_idx'].value_counts().sort_index()\n",
        "\n",
        "# Plot class distribution\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(class_counts.index, class_counts.values)\n",
        "plt.xlabel('Class Label')\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution in Training Data')\n",
        "plt.xticks(range(len(class_counts)))\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "V6YI4go96Q7_",
        "outputId": "3686ed7b-8d03-41dc-8bb9-76a41c8771e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ResNet50"
      ],
      "metadata": {
        "id": "BA95vp-I9ocD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "annotations_path = \"/content/data/task2/train_data/annotations.csv\"\n",
        "labeled_images_path = \"/content/data/task2/train_data/images/labeled\"\n",
        "val_images_path = \"/content/data/task2/val_data\"\n",
        "\n",
        "# Load annotations\n",
        "annotations = pd.read_csv(annotations_path)\n",
        "# Split into train and val\n",
        "train_annotations, val_annotations = train_test_split(annotations, test_size=0.2, stratify=annotations['label_idx'], random_state=42)\n",
        "\n",
        "print(f\"Training samples: {len(train_annotations)}, Validation samples: {len(val_annotations)}\")\n",
        "\n",
        "print(train_annotations.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKwcU9Nu-K6o",
        "outputId": "da0f03cb-6659-44a0-c32f-d3ae684b49cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 40000, Validation samples: 10000\n",
            "                                     renamed_path  label_idx\n",
            "31138  task2/train_data/images/labeled/31138.jpeg         81\n",
            "6179    task2/train_data/images/labeled/6179.jpeg         15\n",
            "8995    task2/train_data/images/labeled/8995.jpeg         23\n",
            "36262  task2/train_data/images/labeled/36262.jpeg         94\n",
            "19536  task2/train_data/images/labeled/19536.jpeg         50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisyLabelsDataset(Dataset):\n",
        "    def __init__(self, annotations, image_dir, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations.iloc[idx, 0]\n",
        "        label = self.annotations.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "\n",
        "# Define transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = NoisyLabelsDataset(train_annotations, \"/content/data\", transform=train_transform)\n",
        "val_dataset = NoisyLabelsDataset(val_annotations, \"/content/data\", transform=val_transform)\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47-C2Qyd9mwy",
        "outputId": "2ee03e64-38aa-4696-f0fc-524fb0627a80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training batches: 1250, Validation batches: 313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        train_acc = 100.0 * correct / total\n",
        "        train_loss = running_loss / len(train_loader.dataset)\n",
        "        val_acc, val_loss = validate_model(model, val_loader, criterion)\n",
        "\n",
        "        # Save the model\n",
        "        model_path = f\"model_epoch_{epoch+1}.pth\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Model saved at {model_path}\")\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "                      f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, \"\n",
        "                      f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "# Validation function\n",
        "def validate_model(model, val_loader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100.0 * correct / total\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    return val_acc, val_loss"
      ],
      "metadata": {
        "id": "-ikMROweBys1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.resnet50(weights=\"DEFAULT\")\n",
        "num_classes = len(annotations['label_idx'].unique())\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "print(num_classes)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Add label smoothing\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "num_epochs = 15\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF4jh6e2-YAR",
        "outputId": "23e7e26c-b47d-43eb-d2f6-a2cfdb57a974"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "Model saved at model_epoch_1.pth\n",
            "Epoch [1/15], Train Loss: 3.1415, Train Acc: 32.09%, Val Loss: 2.0315, Val Acc: 59.53%\n",
            "Model saved at model_epoch_2.pth\n",
            "Epoch [2/15], Train Loss: 2.4828, Train Acc: 47.42%, Val Loss: 1.9042, Val Acc: 63.33%\n",
            "Model saved at model_epoch_3.pth\n",
            "Epoch [3/15], Train Loss: 2.3146, Train Acc: 52.13%, Val Loss: 1.8511, Val Acc: 63.80%\n",
            "Model saved at model_epoch_4.pth\n",
            "Epoch [4/15], Train Loss: 2.2086, Train Acc: 54.51%, Val Loss: 1.8415, Val Acc: 64.47%\n",
            "Model saved at model_epoch_5.pth\n",
            "Epoch [5/15], Train Loss: 2.1272, Train Acc: 56.95%, Val Loss: 1.8091, Val Acc: 64.20%\n",
            "Model saved at model_epoch_6.pth\n",
            "Epoch [6/15], Train Loss: 2.0441, Train Acc: 59.07%, Val Loss: 1.7966, Val Acc: 65.05%\n",
            "Model saved at model_epoch_7.pth\n",
            "Epoch [7/15], Train Loss: 1.9763, Train Acc: 60.98%, Val Loss: 1.7750, Val Acc: 65.85%\n",
            "Model saved at model_epoch_8.pth\n",
            "Epoch [8/15], Train Loss: 1.9086, Train Acc: 62.81%, Val Loss: 1.7649, Val Acc: 65.78%\n",
            "Model saved at model_epoch_9.pth\n",
            "Epoch [9/15], Train Loss: 1.8351, Train Acc: 65.18%, Val Loss: 1.7765, Val Acc: 66.16%\n",
            "Model saved at model_epoch_10.pth\n",
            "Epoch [10/15], Train Loss: 1.7802, Train Acc: 66.84%, Val Loss: 1.7610, Val Acc: 66.15%\n",
            "Model saved at model_epoch_11.pth\n",
            "Epoch [11/15], Train Loss: 1.7305, Train Acc: 68.45%, Val Loss: 1.7528, Val Acc: 66.14%\n",
            "Model saved at model_epoch_12.pth\n",
            "Epoch [12/15], Train Loss: 1.6969, Train Acc: 69.57%, Val Loss: 1.7579, Val Acc: 66.14%\n",
            "Model saved at model_epoch_13.pth\n",
            "Epoch [13/15], Train Loss: 1.6591, Train Acc: 70.82%, Val Loss: 1.7589, Val Acc: 66.74%\n",
            "Model saved at model_epoch_14.pth\n",
            "Epoch [14/15], Train Loss: 1.6421, Train Acc: 71.43%, Val Loss: 1.7567, Val Acc: 66.23%\n",
            "Model saved at model_epoch_15.pth\n",
            "Epoch [15/15], Train Loss: 1.6349, Train Acc: 71.81%, Val Loss: 1.7493, Val Acc: 66.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name\n",
        "\n",
        "val_dir = \"/content/data/task2/val_data\"\n",
        "\n",
        "# Create the validation dataset and DataLoader\n",
        "test_dataset = UnlabeledDataset(val_dir, transform=val_transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# List to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Inference loop\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = outputs.max(1)  # Get the index of the max log-probability\n",
        "\n",
        "        # Append predictions with image names\n",
        "        for img_name, label in zip(img_names, predicted.cpu().numpy()):\n",
        "            predictions.append({\n",
        "                \"ID\": f\"task2/val_data/{img_name}\",\n",
        "                \"label\": label\n",
        "            })\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "\n",
        "# Save to CSV\n",
        "submission_file = \"submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "\n",
        "print(f\"Submission file saved as {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "t4eTVPXaOdvk",
        "outputId": "c036bbf0-4c90-4269-d790-dac609c57355"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'val_transform' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-49ab683753d3>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Create the validation dataset and DataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnlabeledDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'val_transform' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DivideMix"
      ],
      "metadata": {
        "id": "8aa2edf7_99X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabeledDataset(Dataset):\n",
        "    def __init__(self, annotations, image_dir, transform=None):\n",
        "        self.annotations = annotations\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.annotations.iloc[idx, 0]\n",
        "        label = self.annotations.iloc[idx, 1]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "class TestDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, annotations, image_dir, mode, transform=None, pred=None, prob=None):\n",
        "        self.annotations = annotations\n",
        "        self.image_dir = image_dir\n",
        "        self.mode = mode\n",
        "        self.transform = transform\n",
        "        self.pred = pred\n",
        "        self.prob = prob\n",
        "\n",
        "        if pred is not None:\n",
        "            pred = np.array(pred, dtype=bool)  # Ensure `pred` is a numpy array of boolean values\n",
        "        if prob is not None:\n",
        "            prob = np.array(prob)  # Ensure `prob` is a numpy array\n",
        "\n",
        "        if mode == 'all':\n",
        "            self.data = self.annotations\n",
        "        elif mode == 'labeled':\n",
        "            # Filter clean samples based on `pred`\n",
        "            self.data = self.annotations[pred].reset_index(drop=True)\n",
        "            self.prob = prob[pred]\n",
        "        elif mode == 'unlabeled':\n",
        "            # Filter noisy samples\n",
        "            self.data = self.annotations[~pred].reset_index(drop=True)\n",
        "            self.prob = prob[~pred]\n",
        "        elif mode in ['test', 'val']:\n",
        "            self.data = self.annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.iloc[idx, 0]\n",
        "        label = self.data.iloc[idx, 1] if self.mode in ['labeled', 'all'] else None\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img1 = self.transform(image)\n",
        "            img2 = self.transform(image)  # Create duplicate transformations\n",
        "\n",
        "        if self.mode == 'labeled':\n",
        "            prob = self.prob[idx]\n",
        "            return img1, img2, label, prob\n",
        "        elif self.mode == 'unlabeled':\n",
        "            return img1, img2\n",
        "        elif self.mode == 'all':\n",
        "            return img1, label\n",
        "        elif self.mode in ['test', 'val']:\n",
        "            return img1, label\n",
        "\n",
        "class CustomDataloader:\n",
        "    def __init__(self, batch_size, num_workers):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.transform_train = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.RandomCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.6959, 0.6537, 0.6371),(0.3113, 0.3192, 0.3214)),\n",
        "            ])\n",
        "        self.transform_test = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.6959, 0.6537, 0.6371),(0.3113, 0.3192, 0.3214)),\n",
        "            ])\n",
        "        self.annotations = annotations\n",
        "\n",
        "    def run(self, mode, annotations, pred=[],prob=[]):\n",
        "      if mode == 'warmup':\n",
        "            dataset = CustomDataset(annotations, \"/content/data\", mode='all', transform=self.transform_train)\n",
        "            loader = DataLoader(dataset, batch_size=self.batch_size * 2, shuffle=True, num_workers=self.num_workers)\n",
        "            return loader\n",
        "      elif mode == 'train':\n",
        "        labeled_dataset = CustomDataset(\n",
        "                annotations, \"/content/data\", mode='labeled',\n",
        "                transform=self.transform_train, pred=pred, prob=prob\n",
        "            )\n",
        "        unlabeled_dataset = CustomDataset(\n",
        "                annotations, \"/content/data\", mode='unlabeled',\n",
        "                transform=self.transform_train, pred=pred, prob=prob\n",
        "            )\n",
        "        labeled_loader = DataLoader(labeled_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "        unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
        "        return labeled_loader, unlabeled_loader\n",
        "\n",
        "      elif mode == 'eval_train':\n",
        "            eval_dataset = LabeledDataset(\n",
        "                annotations=annotations,\n",
        "                image_dir=\"/content/data\",\n",
        "                transform=self.transform_test,\n",
        "            )\n",
        "            eval_loader = DataLoader(\n",
        "                dataset=eval_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=self.num_workers,\n",
        "            )\n",
        "            return eval_loader\n",
        "\n",
        "      elif mode == 'val':\n",
        "            val_dataset = LabeledDataset(\n",
        "                annotations=annotations,\n",
        "                image_dir=\"/content/data\",\n",
        "                transform=self.transform_test\n",
        "            )\n",
        "\n",
        "            val_loader = DataLoader(\n",
        "                dataset=val_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=self.num_workers\n",
        "            )\n",
        "            return val_loader\n",
        "\n",
        "      elif mode == 'test':\n",
        "            test_dataset = TestDataset(\n",
        "                image_dir=\"/content/data/task2/val_data\",  # Assuming test images are in the unlabeled path\n",
        "                transform=self.transform_test,\n",
        "            )\n",
        "\n",
        "            test_loader = DataLoader(\n",
        "                dataset=test_dataset,\n",
        "                batch_size=self.batch_size,\n",
        "                shuffle=False,\n",
        "                num_workers=self.num_workers,\n",
        "            )\n",
        "            return test_loader\n",
        "\n",
        "\n",
        "annotations_path = \"/content/data/task2/train_data/annotations.csv\"\n",
        "labeled_images_path = \"/content/data/task2/train_data/images/labeled\"\n",
        "val_images_path = \"/content/data/task2/val_data\"\n",
        "\n",
        "# Load annotations\n",
        "annotations = pd.read_csv(annotations_path)\n",
        "# Split into train and val\n",
        "train_annotations, val_annotations = train_test_split(annotations, test_size=0.2, stratify=annotations['label_idx'], random_state=42)\n",
        "\n",
        "# Initialize the custom data loader\n",
        "custom_loader = CustomDataloader(\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        ")\n",
        "\n",
        "\n",
        "# Warm-up phase\n",
        "warmup_loader = custom_loader.run('warmup', annotations=train_annotations)\n",
        "\n",
        "num_classes = len(annotations['label_idx'].unique())\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9DNnmpBLFJ",
        "outputId": "e6f8ba72-ae09-44d6-8006-efec10a9ce91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model = models.resnet50(weights=\"DEFAULT\")\n",
        "    model.fc = nn.Linear(2048,num_classes)\n",
        "    model = model.cuda()\n",
        "    return model\n",
        "\n",
        "class NegEntropy(object):\n",
        "    def __call__(self,outputs):\n",
        "        probs = torch.softmax(outputs, dim=1)\n",
        "        return torch.mean(torch.sum(probs.log()*probs, dim=1))\n",
        "\n",
        "def warmup(net,optimizer,dataloader):\n",
        "    net.train()\n",
        "    running_loss = 0.0  # Initialize running loss\n",
        "\n",
        "    for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = CEloss(outputs, labels)\n",
        "\n",
        "        penalty = conf_penalty(outputs)\n",
        "        total_loss = loss + penalty      # Combine loss and penalty\n",
        "        total_loss.backward()            # Backpropagation\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += total_loss.item()  # Accumulate total loss\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write('|Warm-up: Iter[%3d/%3d]\\t CE-loss: %.4f  Conf-Penalty: %.4f'\n",
        "                %(batch_idx+1, len(dataloader), loss.item(), penalty.item()))\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    print(f\"\\nWarm-up completed. Average Loss: {running_loss / len(dataloader):.4f}\")\n",
        "\n",
        "def eval_train(model):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch_idx, (inputs, targets) in enumerate(eval_loader):  # No `path` here\n",
        "          inputs, targets = inputs.cuda(), targets.cuda()\n",
        "          outputs = model(inputs)\n",
        "          loss = CE(outputs, targets)  # Per-sample loss\n",
        "          losses.extend(loss.cpu().numpy())  # Store losses\n",
        "\n",
        "          sys.stdout.write('\\r')\n",
        "          sys.stdout.write('| Evaluating loss Iter [%3d/%3d]\\t'\n",
        "                            % (batch_idx + 1, len(eval_loader)))\n",
        "          sys.stdout.flush()\n",
        "\n",
        "    losses = np.array(losses)\n",
        "    # Normalize losses\n",
        "    losses = (losses - losses.min()) / (losses.max() - losses.min())\n",
        "    losses = losses.reshape(-1, 1)\n",
        "\n",
        "    gmm = GaussianMixture(n_components=2, max_iter=10, reg_covar=5e-4, tol=1e-2)\n",
        "    gmm.fit(losses)\n",
        "    prob = gmm.predict_proba(losses)  # Probabilities for each component\n",
        "    prob = prob[:, gmm.means_.argmin()]  # Probability of being clean\n",
        "\n",
        "    return prob\n",
        "\n",
        "def train(epoch, net, net2, optimizer, labeled_trainloader, unlabeled_trainloader, num_classes, temperature, alpha):\n",
        "    net.train()\n",
        "    net2.eval()  # Fix one network and train the other\n",
        "\n",
        "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "    num_iter = len(labeled_trainloader)\n",
        "\n",
        "    for batch_idx, (inputs_x, inputs_x2, labels_x, w_x) in enumerate(labeled_trainloader):\n",
        "        try:\n",
        "            inputs_u, inputs_u2 = next(unlabeled_train_iter)\n",
        "        except StopIteration:\n",
        "            unlabeled_train_iter = iter(unlabeled_trainloader)\n",
        "            inputs_u, inputs_u2 = next(unlabeled_train_iter)\n",
        "\n",
        "        batch_size = inputs_x.size(0)\n",
        "\n",
        "        # Transform labels to one-hot encoding\n",
        "        labels_x = torch.zeros(batch_size, num_classes).scatter_(1, labels_x.view(-1, 1), 1)\n",
        "        w_x = w_x.view(-1, 1).float()\n",
        "\n",
        "         # Move to GPU\n",
        "        inputs_x, inputs_x2, labels_x, w_x = inputs_x.cuda(), inputs_x2.cuda(), labels_x.cuda(), w_x.cuda()\n",
        "        inputs_u, inputs_u2 = inputs_u.cuda(), inputs_u2.cuda()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Co-guessing on unlabeled samples\n",
        "            outputs_u11 = net(inputs_u)\n",
        "            outputs_u12 = net(inputs_u2)\n",
        "            outputs_u21 = net2(inputs_u)\n",
        "            outputs_u22 = net2(inputs_u2)\n",
        "\n",
        "            pu = (torch.softmax(outputs_u11, dim=1) + torch.softmax(outputs_u12, dim=1) +\n",
        "                  torch.softmax(outputs_u21, dim=1) + torch.softmax(outputs_u22, dim=1)) / 4\n",
        "            ptu = pu**(1 / temperature)  # Temperature sharpening\n",
        "            targets_u = (ptu / ptu.sum(dim=1, keepdim=True)).detach()  # Normalize\n",
        "\n",
        "            # Refinement on labeled samples\n",
        "            outputs_x = net(inputs_x)\n",
        "            outputs_x2 = net(inputs_x2)\n",
        "\n",
        "            px = (torch.softmax(outputs_x, dim=1) + torch.softmax(outputs_x2, dim=1)) / 2\n",
        "            px = w_x * labels_x + (1 - w_x) * px\n",
        "            ptx = px**(1 / temperature)  # Temperature sharpening\n",
        "            targets_x = (ptx / ptx.sum(dim=1, keepdim=True)).detach()\n",
        "\n",
        "        # MixMatch (Mix inputs and targets)\n",
        "        l = np.random.beta(alpha, alpha)\n",
        "        l = max(l, 1 - l)\n",
        "\n",
        "        all_inputs = torch.cat([inputs_x, inputs_x2, inputs_u, inputs_u2], dim=0)\n",
        "        all_targets = torch.cat([targets_x, targets_x, targets_u, targets_u], dim=0)\n",
        "\n",
        "        idx = torch.randperm(all_inputs.size(0))\n",
        "        input_a, input_b = all_inputs, all_inputs[idx]\n",
        "        target_a, target_b = all_targets, all_targets[idx]\n",
        "\n",
        "        mixed_input = l * input_a[:batch_size * 2] + (1 - l) * input_b[:batch_size * 2]\n",
        "        mixed_target = l * target_a[:batch_size * 2] + (1 - l) * target_b[:batch_size * 2]\n",
        "\n",
        "        # Forward pass\n",
        "        logits = net(mixed_input)\n",
        "        Lx = -torch.mean(torch.sum(F.log_softmax(logits, dim=1) * mixed_target, dim=1))\n",
        "\n",
        "        # Regularization\n",
        "        prior = torch.ones(num_classes) / num_classes\n",
        "        prior = prior.cuda()\n",
        "        pred_mean = torch.softmax(logits, dim=1).mean(0)\n",
        "        penalty = torch.sum(prior * torch.log(prior / pred_mean))\n",
        "\n",
        "        loss = Lx + penalty\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Logging\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(f'Epoch [{epoch}] Iter[{batch_idx + 1}/{num_iter}] Labeled loss: {Lx.item():.4f}')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "# Define a simple validation function\n",
        "def validate(epoch, net1, net2, val_loader):\n",
        "    net1.eval()\n",
        "    net2.eval()\n",
        "    total_correct_net1, total_correct_net2 = 0, 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "            # Net1 predictions\n",
        "            outputs_net1 = net1(inputs)\n",
        "            _, predicted_net1 = outputs_net1.max(1)\n",
        "            total_correct_net1 += (predicted_net1 == labels).sum().item()\n",
        "\n",
        "            # Net2 predictions\n",
        "            outputs_net2 = net2(inputs)\n",
        "            _, predicted_net2 = outputs_net2.max(1)\n",
        "            total_correct_net2 += (predicted_net2 == labels).sum().item()\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    acc_net1 = 100.0 * total_correct_net1 / total_samples\n",
        "    acc_net2 = 100.0 * total_correct_net2 / total_samples\n",
        "\n",
        "    print(f\"Epoch [{epoch}] Validation Accuracy - Net1: {acc_net1:.2f}%, Net2: {acc_net2:.2f}%\")\n",
        "    return acc_net1, acc_net2\n"
      ],
      "metadata": {
        "id": "EgDOiFsaKwRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('| Building net')\n",
        "net1 = create_model()\n",
        "net2 = create_model()\n",
        "\n",
        "optimizer1 = optim.SGD(net1.parameters(), lr=0.002, momentum=0.9, weight_decay=1e-3)\n",
        "optimizer2 = optim.SGD(net2.parameters(), lr=0.002, momentum=0.9, weight_decay=1e-3)\n",
        "\n",
        "\n",
        "CE = nn.CrossEntropyLoss(reduction='none')\n",
        "CEloss = nn.CrossEntropyLoss()\n",
        "conf_penalty = NegEntropy()\n",
        "\n",
        "best_acc_net1 = 0\n",
        "best_acc_net2 = 0\n",
        "\n",
        "for epoch in range(10 + 1):\n",
        "  if epoch < 1:  # Warm-up phase\n",
        "        train_loader = custom_loader.run('warmup', annotations=train_annotations)\n",
        "        print('Warmup Net1')\n",
        "        warmup(net1, optimizer1, train_loader)\n",
        "        train_loader = custom_loader.run('warmup', annotations=train_annotations)\n",
        "        print('\\nWarmup Net2')\n",
        "        warmup(net2, optimizer2, train_loader)\n",
        "  else:\n",
        "      print('\\n==== Evaluate Loss for Net1 ====')\n",
        "      eval_loader = custom_loader.run('eval_train', annotations=train_annotations)\n",
        "      prob1 = eval_train(net1)\n",
        "\n",
        "      print('\\n==== Evaluate Loss for Net2 ====')\n",
        "      eval_loader = custom_loader.run('eval_train', annotations=train_annotations)\n",
        "      prob2 = eval_train(net2)\n",
        "\n",
        "      pred1 = (prob1 > 0.5)  # Threshold for clean samples for Net1\n",
        "      pred2 = (prob2 > 0.5)  # Threshold for clean samples for Net2\n",
        "      print('\\n\\nTrain Net1')\n",
        "      labeled_trainloader, unlabeled_trainloader = custom_loader.run('train', annotations=train_annotations, pred=pred2, prob=prob2)  # Co-divide\n",
        "      train(epoch, net1, net2, optimizer1, labeled_trainloader, unlabeled_trainloader, num_classes, temperature=0.5, alpha=0.5)\n",
        "\n",
        "      print('\\nTrain Net2')\n",
        "      labeled_trainloader, unlabeled_trainloader = custom_loader.run('train', annotations=train_annotations, pred=pred1, prob=prob1)  # Co-divide\n",
        "      train(epoch, net2, net1, optimizer2, labeled_trainloader, unlabeled_trainloader, num_classes, temperature=0.5, alpha=0.5)\n",
        "\n",
        "  # Validation phase\n",
        "  print('\\n==== Validation Phase ====')\n",
        "  val_loader = custom_loader.run('val', annotations=val_annotations)\n",
        "  acc_net1, acc_net2 = validate(epoch, net1, net2, val_loader)\n",
        "\n",
        "  # Save best models based on validation accuracy\n",
        "  if acc_net1 > best_acc_net1:\n",
        "      best_acc_net1 = acc_net1\n",
        "      torch.save(net1.state_dict(), f'best_net1_epoch_{epoch}.pth')\n",
        "\n",
        "  if acc_net2 > best_acc_net2:\n",
        "      best_acc_net2 = acc_net2\n",
        "      torch.save(net2.state_dict(), f'best_net2_epoch_{epoch}.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOQlLB1Wb8Ht",
        "outputId": "decce4e6-a87a-4cef-8c47-0c1aafdb377e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Building net\n",
            "Warmup Net1\n",
            "|Warm-up: Iter[625/625]\t CE-loss: 2.3909  Conf-Penalty: -4.0738\n",
            "Warm-up completed. Average Loss: -0.9977\n",
            "\n",
            "Warmup Net2\n",
            "|Warm-up: Iter[625/625]\t CE-loss: 2.4654  Conf-Penalty: -4.0707\n",
            "Warm-up completed. Average Loss: -0.9605\n",
            "\n",
            "==== Validation Phase ====\n",
            "Epoch [0] Validation Accuracy - Net1: 62.63%, Net2: 61.45%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [1] Iter[805/805] Labeled loss: 3.3000\n",
            "Train Net2\n",
            "Epoch [1] Iter[818/818] Labeled loss: 1.7829\n",
            "==== Validation Phase ====\n",
            "Epoch [1] Validation Accuracy - Net1: 64.65%, Net2: 65.12%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [2] Iter[833/833] Labeled loss: 1.3673\n",
            "Train Net2\n",
            "Epoch [2] Iter[900/900] Labeled loss: 3.4605\n",
            "==== Validation Phase ====\n",
            "Epoch [2] Validation Accuracy - Net1: 67.14%, Net2: 67.09%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [3] Iter[860/860] Labeled loss: 1.2100\n",
            "Train Net2\n",
            "Epoch [3] Iter[839/839] Labeled loss: 2.8274\n",
            "==== Validation Phase ====\n",
            "Epoch [3] Validation Accuracy - Net1: 68.06%, Net2: 67.51%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [4] Iter[850/850] Labeled loss: 2.2316\n",
            "Train Net2\n",
            "Epoch [4] Iter[864/864] Labeled loss: 1.4445\n",
            "==== Validation Phase ====\n",
            "Epoch [4] Validation Accuracy - Net1: 67.90%, Net2: 68.20%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [5] Iter[882/882] Labeled loss: 2.7005\n",
            "Train Net2\n",
            "Epoch [5] Iter[946/946] Labeled loss: 0.9959\n",
            "==== Validation Phase ====\n",
            "Epoch [5] Validation Accuracy - Net1: 68.41%, Net2: 68.75%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [6] Iter[916/916] Labeled loss: 1.1969\n",
            "Train Net2\n",
            "Epoch [6] Iter[882/882] Labeled loss: 3.0404\n",
            "==== Validation Phase ====\n",
            "Epoch [6] Validation Accuracy - Net1: 68.78%, Net2: 68.53%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [7] Iter[906/906] Labeled loss: 1.3162\n",
            "Train Net2\n",
            "Epoch [7] Iter[901/901] Labeled loss: 1.0015\n",
            "==== Validation Phase ====\n",
            "Epoch [7] Validation Accuracy - Net1: 68.66%, Net2: 68.95%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [8] Iter[913/913] Labeled loss: 2.4664\n",
            "Train Net2\n",
            "Epoch [8] Iter[918/918] Labeled loss: 3.5978\n",
            "==== Validation Phase ====\n",
            "Epoch [8] Validation Accuracy - Net1: 69.19%, Net2: 69.19%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [9] Iter[914/914] Labeled loss: 0.6626\n",
            "Train Net2\n",
            "Epoch [9] Iter[915/915] Labeled loss: 0.6032\n",
            "==== Validation Phase ====\n",
            "Epoch [9] Validation Accuracy - Net1: 69.07%, Net2: 69.19%\n",
            "\n",
            "==== Evaluate Loss for Net1 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "==== Evaluate Loss for Net2 ====\n",
            "| Evaluating loss Iter [1250/1250]\t\n",
            "\n",
            "Train Net1\n",
            "Epoch [10] Iter[923/923] Labeled loss: 4.5555\n",
            "Train Net2\n",
            "Epoch [10] Iter[921/921] Labeled loss: 2.4315\n",
            "==== Validation Phase ====\n",
            "Epoch [10] Validation Accuracy - Net1: 66.56%, Net2: 69.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class UnlabeledDataset(Dataset):\n",
        "    def __init__(self, image_dir, transform=None):\n",
        "        self.image_dir = image_dir\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.images[idx]\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, img_name\n",
        "\n",
        "val_dir = \"/content/data/task2/val_data\"\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.6959, 0.6537, 0.6371),(0.3113, 0.3192, 0.3214)),\n",
        "            ])\n",
        "\n",
        "# Create the validation dataset and DataLoader\n",
        "test_dataset = UnlabeledDataset(val_dir, transform=transform_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net1.load_state_dict(torch.load('best_net1_epoch_8.pth'))\n",
        "net2.load_state_dict(torch.load('best_net2_epoch_10.pth'))\n",
        "\n",
        "net1.eval()\n",
        "net2.eval()\n",
        "\n",
        "# List to store predictions\n",
        "predictions = []\n",
        "\n",
        "# Inference loop\n",
        "with torch.no_grad():\n",
        "    for images, img_names in test_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Predictions from both networks\n",
        "        outputs_net1 = net1(images)\n",
        "        outputs_net2 = net2(images)\n",
        "\n",
        "        # Softmax to get probabilities\n",
        "        probs_net1 = torch.softmax(outputs_net1, dim=1)\n",
        "        probs_net2 = torch.softmax(outputs_net2, dim=1)\n",
        "\n",
        "        # Combine predictions (e.g., average probabilities)\n",
        "        combined_probs = (probs_net1 + probs_net2) / 2\n",
        "\n",
        "        # Final predictions (get class with highest probability)\n",
        "        _, predicted_classes = combined_probs.max(1)\n",
        "\n",
        "        # Append predictions with image names\n",
        "        for img_name, label in zip(img_names, predicted_classes.cpu().numpy()):\n",
        "            predictions.append({\n",
        "                \"ID\": f\"task2/val_data/{img_name}\",\n",
        "                \"label\": label\n",
        "            })\n",
        "\n",
        "\n",
        "# Convert predictions to a DataFrame\n",
        "submission_df = pd.DataFrame(predictions)\n",
        "\n",
        "# Save to CSV\n",
        "submission_file = \"submission.csv\"\n",
        "submission_df.to_csv(submission_file, index=False)\n",
        "\n",
        "print(f\"Submission file saved as {submission_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3I_z62d3X79",
        "outputId": "3cffed42-3174-4768-ba5b-e5223759d59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-3a2a2486699b>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net1.load_state_dict(torch.load('best_net1_epoch_8.pth'))\n",
            "<ipython-input-13-3a2a2486699b>:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  net2.load_state_dict(torch.load('best_net2_epoch_10.pth'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission file saved as submission.csv\n"
          ]
        }
      ]
    }
  ]
}